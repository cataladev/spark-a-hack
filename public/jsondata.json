[
    {
        "description": "\n\n\n\n\n\nR.E.A.C.T. is an AI-driven telecom system for remote areas. Powered by renewable energy, it\u2019s affordable, easy to deploy, and provides reliable connectivity for education, healthcare, and more.\n\n\n\n\n\nThe inspiration came from witnessing the stark contrast between the connected and the disconnected. The quote states \"I have a million dollar dream in a village which has never seen a million dollar dream.\" played a very big role in inspiring us to allow every individual living in the remotest of areas to dream.\nR.E.A.C.T. is a robust telecommunications solution designed to provide reliable connectivity to remote areas.It achieves this through a combination of renewable energy systems, AI-powered mesh networking, and advanced Li-Fi technology, all working together to form a self-sufficient and adaptable communication infrastructure.\nIt\u2019s more than just a network\u2014it\u2019s a lifeline for those who need it most.\nBuilding R.E.A.C.T. was nothing less than a week-long full of discussion meetings and surviving on ramen. We knew from the start that if we were participating, we might as well go for the first price. After countless discussions, meetings, arguments and nothing short of 20 ramen packs; here's how it all came together:\nRenewable Energy Backbone: We integrated solar panels and wind turbines, making the system self-sufficient even in the harshest conditions. It gathers and stores energy, keeping essential services running 24/7. In cases where this wasn't feasible, we allowed the system to be dependent on other kind of renewable energy sources.\nAI-Optimised Mesh Networking: The core of R.E.A.C.T. is its intelligent mesh network. Using AI, the system continuously monitors and adapts to ensure optimal bandwidth, predict network bottlenecks, and reroute traffic for seamless connectivity.\nLi-Fi Technology: We knew traditional Wi-Fi wouldn\u2019t cut it in areas with high interference, so we implemented Li-Fi, offering faster, more secure data transmission even in remote areas.\nModular Design: From day one, we built R.E.A.C.T. to be plug-and-play. It\u2019s light, modular, and easy to assemble, meaning communities can deploy and maintain it without needing experts.\nOur team grappled with:\nUnforgiving Environments: Designing hardware that could withstand extreme weather conditions was a tough battle. From scorching heat to monsoons, we had to ensure the system was tough enough to endure. We ended up deciding to keep our materials flexible. The outer casing is depended upon the region it is going to be settled in and comes standard with an aluminu\nBalancing Cutting-Edge Tech with Affordability: Incorporating features like AI and Li-Fi without pushing costs through the roof was a constant balancing act. We wanted innovation, but we also wanted accessibility for the communities we were serving.\nCommunity Engagement: Building the technology was only half the battle. Making sure it could be operated by locals with minimal training was another challenge. We needed the system to be as intuitive as it was powerful.\nWe\u2019re incredibly proud of what R.E.A.C.T. has achieved so far:\nCreating an A.I bot locally to work for our needs\nBeing able to see our design come to life with a 3D model\nAI-driven Connectivity: Our AI-optimised mesh network has exceeded expectations, allowing seamless connectivity in areas where traditional systems would have failed. We ran multiple test benches and it performed way better than our expectations.\nThe journey of creating R.E.A.C.T. has taught us many valuable lessons:\nResilience is Key: Both in technology and in the people we aim to help.\nTechnology Alone Isn\u2019t Enough: The tech we built is powerful, but what makes R.E.A.C.T. truly special is the people who use it. We learned that human-centered design is crucial\u2014what good is innovation if it\u2019s not accessible?\nThe journey of R.E.A.C.T. is far from over. In fact, we\u2019re just getting started:\nScaling Globally: We aim to bring R.E.A.C.T. to even more remote communities across the world. From Africa to Southeast Asia, our goal is to ensure that everyone, no matter where they live, has access to reliable communication and services.\nFurther Enhancing AI Capabilities: Our AI-optimised mesh network will continue to evolve, with even more predictive analytics and bandwidth management to make sure connectivity is always top-notch.\nExpanding Services: Beyond healthcare and education, we\u2019re exploring new ways for R.E.A.C.T. to support communities\u2014whether it\u2019s agricultural data for farmers or disaster relief coordination, the possibilities are endless.\nGrid The Globe \u2013 GDSC \u00d7 EWB USYD 2024\nTweaked the presentation, came up with the foundation of our ideation leading the path for the project. Created the concept report and did all the flowcharts. Helped in creating the AI and trained it with hypothetical data.\nCompleting video editing and conducting initial research\nMaterial Planning\nIdentify materials and estimate quantities\nSet timelines and coordinate with suppliers\nManage inventory, control quality, and minimize waste\nOptimize costs\nPresentation\nSet objectives, structure content, and engage audience\nUse clear language and rehearse delivery\nHandle questions and summarize key points\nProduct Concept\nMeets demand and drives branding\nAkrishh Bali started this project \u2014 a day ago\nLeave feedback in the comments!\nView previous comments\nRaju D V \u00b7 about 23 hours ago\nGreat job Akrish Bali\nRakesh Bali \u00b7 about 23 hours ago\nAll the economically weak areas in this world can take advantage of this, whether it is the field of education or the field of development. It will also prove beneficial in medicine, those areas Where the doctor cannot reach, through telecommunication that doctor will connect with the people living in distant areas.\nKingshuk Das \u00b7 about 12 hours ago\nWow Krish, great going. Glad to know about this project.\nDisha Jain \u00b7 about 10 hours ago\nSuch an inspiring project!\nrahul sharma \u00b7 about an hour ago\nwow\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nHigh-altitude, lightweight satellite\n\n\n\nThe inspiration behind M.E.C.K.S (Mobile Elevated Communication and Knowledge Satellite) was the need to address the significant digital divide between urban and rural areas. While urban populations benefit from advanced telecommunications infrastructure, many remote regions, such as Northern Australia, East Malaysia, and parts of Africa, remain disconnected from vital services. These areas face challenges like lack of infrastructure, electricity, and transportation, which complicates the deployment of traditional communications technology.\nThe idea for this project came from the desire to create an affordable, scalable, and accessible solution that could bring reliable internet and telecommunications services to underserved regions. High-altitude platforms and aerial technology offered an exciting opportunity to provide satellite-like coverage, but without the high costs associated with traditional satellite systems.\nWe approached the M.E.C.K.S Satellite On Earth project by first conducting thorough research on existing telecommunications challenges in remote regions. We identified the limitations of traditional infrastructure, such as the limited range of cell towers and the prohibitive costs of satellite-based systems. With this foundation, we moved on to:\nDesigning the Aerial Platform: We focused on creating a lightweight, durable structure optimized for high-altitude endurance. This involved selecting materials such as aluminum 6061 for the main frame, carbon fiber for the wings, and titanium for additional structural integrity. Simulating Aerodynamics and Coverage: Using software simulations, we designed the curved wing structure for optimal lift and minimal energy consumption. This allowed the platform to cover wide areas while remaining efficient in high-altitude conditions. Telecommunications Integration: We researched and integrated low-cost wireless communication systems capable of long-range coverage to ensure the platform could effectively serve remote communities without needing physical infrastructure like cell towers. Cost and Feasibility Analysis: A significant part of the build process was performing a cost analysis on the materials and telecommunications technology to ensure that the solution would be both effective and affordable for large-scale deployment in underdeveloped regions.\nWeather Interference: Designing a system that could function reliably in diverse weather conditions was a major technical hurdle. High winds, storms, and temperature variations had to be accounted for in the aerodynamic design and communications equipment. Bandwidth Limitations: Ensuring sufficient bandwidth and connectivity for large user bases, especially in emergencies, required careful planning. We explored options for optimizing data transmission and dealing with the shared bandwidth problem inherent in wireless platforms. Regulatory Issues: Navigating airspace regulations and spectrum allocation was challenging. Ensuring compliance with international aviation and telecommunications standards like ICAO and ITU added complexity to the deployment plan. Sustainability: Achieving long-term energy efficiency while keeping costs low was crucial. We had to balance material choices and communication technologies to ensure the platform could operate continuously without excessive energy demands.\nMultidisciplinary Collaboration: We learned that combining elements of aerospace engineering, telecommunications, and socio-economic development is essential for creating solutions that go beyond technology and offer real-world impact. Importance of Adaptability: As we tackled weather and bandwidth limitations, we realized that adaptability\u2014both in terms of technology and deployment strategy\u2014was key to overcoming these challenges. Regulatory Considerations: Understanding the complex web of international and national regulations governing telecommunications and airspace was an eye-opener and taught us the importance of early stakeholder engagement.\nField Testing: The next step is to test the M.E.C.K.S platform in real-world conditions, focusing on challenging terrains and climates to optimize performance under various conditions. Improved Bandwidth Management: We plan to work on advanced bandwidth optimization techniques to handle higher user densities without affecting performance. Wider Deployment: After successful field testing, the goal is to collaborate with NGOs, governments, and telecommunications companies to deploy the M.E.C.K.S system in remote regions globally, ensuring access to education, healthcare, and emergency services. Disaster Relief Applications: Another key focus will be refining the model for rapid deployment during natural disasters, providing immediate communication support for emergency teams and affected populations.\nGrid The Globe \u2013 GDSC \u00d7 EWB USYD 2024\nKamilia Nafis posted an update \u2014 a day ago\nGreat work guys!\nView previous comments\nSunil Swami \u00b7 a day ago\nExcellent thought\nNayan Kumar Vyas \u00b7 a day ago\nExcellent efforts\nSWASTIK \u00b7 about 22 hours ago\nTop notch efforts\nDiya \u00b7 about 20 hours ago\nGreat!!\nArgha Ghosh \u00b7 about 19 hours ago\nBrilliant thought process\nLog in or sign up for Devpost to join the conversation.\nKamilia Nafis started this project \u2014 a day ago\nLeave feedback in the comments!\nView previous comments\nlove, hin \u00b7 a day ago\nExcellent project\njahnvi patni \u00b7 a day ago\nExcellent work!\nSOFEA IRDINA ATHIRAH BINTI SUHAIMI . \u00b7 a day ago\nGreat project\nAbhimanyu Joshi \u00b7 a day ago\nKeep the great work up\nDiya \u00b7 about 20 hours ago\nAmazing\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\n\"LA-Wifi: Connecting communities, no matter how remote it is\"\nThe inspiration for this project comes from the growing need for reliable, rapid-deployment communication systems in remote and disaster-stricken areas. Communities without access to traditional networks face barriers in accessing essential services, especially in times of crisis. The aim is to close the digital divide by providing an affordable, scalable, and durable solution that can deliver broad Wi-Fi coverage to entire communities.\nWhat it does The system provides wide-range Wi-Fi connectivity using phased array antennas, offering coverage over large geographic areas with minimal infrastructure. It ensures that remote communities have access to vital services such as healthcare, education, emergency communications, and social connections. Designed for quick deployment, it brings internet access to areas where traditional telecom services are unavailable or damaged.\nHow we built it The system is based on phased array antenna technology, allowing for directional and broad Wi-Fi coverage. The design integrates multiplexers and a central modem that connects either through ADSL or satellite services (e.g., Starlink or Sky Muster). We focused on making the antennas compact and mobile so that they can be easily transported and deployed from the back of a utility vehicle.\nChallenges we ran into Some challenges include:\nBackhaul limitations: Handling bandwidth bottlenecks when an entire community relies on a single access point. Cost considerations: Balancing affordability with the high cost of advanced antennas and satellite services. Deployment in extreme conditions: Ensuring the system can withstand harsh weather or environmental challenges typical in remote locations. Network congestion: Managing multiple users on limited bandwidth in peak usage times. Accomplishments that we're proud of Successfully designing a compact, mobile Wi-Fi system that can be deployed rapidly in emergency or remote situations. Using phased array technology to provide high bandwidth over large areas with minimal infrastructure. Creating a solution that directly addresses the digital gap in underserved communities, providing a lifeline for critical services like healthcare and education. What we learned The importance of backhaul flexibility, such as utilizing both satellite and ADSL options to ensure continuous connectivity. User accessibility and simplicity are key in designing tech for communities with limited technical expertise. How vital telecommunications are in remote areas for improving the quality of life, particularly for First Nations communities and disaster recovery efforts. What's next for Rapid Mobile Large Area Wi-Fi The next steps include:\nScaling the system to cover larger communities or regions by adding additional modems and optimizing bandwidth use. Partnering with government and telecommunications companies to make the solution widely available in rural and remote areas. Exploring renewable energy solutions like solar-powered units to ensure long-term sustainability and further reduce costs. Testing and refining the system for deployment in different environments, including tropical and desert regions, to ensure durability in all conditions.\nGrid The Globe \u2013 GDSC \u00d7 EWB USYD 2024\nFinn Panko-Jones started this project \u2014 a day ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nInside of a durable fire-proof casing, we\u2019ve designed a minimalistic system that can be easily deployed before an emergency, transmit and receive a signal in a grid-style fashion.\nInside of a durable fire-proof casing, we\u2019ve designed a minimalistic system that can be easily deployed before an emergency, transmit and receive a signal in a grid-style fashion. These modules can be mounted to trees and are deliberately designed with no moving parts.\nGrid The Globe \u2013 GDSC \u00d7 EWB USYD 2024\nTameem Khan started this project \u2014 a day ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nOur project is designed to improve student lives by analyzing text examples (such as essays) and comparing it to a rubric to grade student work.\nOur project is a tool for students that allows them to reflect on their work, such as essays, using an AI tool that checks their work against a provided rubric.\nGRAS uses the OpenAI API for GPT4-turbo and Vision to process the rubric into plain text, then it uses the GPT4-turbo API to analyze the user's essay and provide them with a percentage and some feedback on how to make their assignment better aligned to the rubric.\nA primary challenge we ran into was configuring our backend to properly interface with the OpenAI API properly, as it had a lot of room for error and needed careful prompting to get optimized results.\nWe are proud of our intuitive UI that allows users to simply upload an image and paste their assignment to have the API and our program to process their grade.\nWe learned a lot about using GPT models and troubleshooting them.\nGRAS is currently quite simple, a future update would include more versatility for file types, a more nature oriented UI, and more detailed results.\nyvrHacks 2024\nOur project is designed to improve student lives by analyzing text examples (such as essays) and comparing it to a rubric to grade student work. This project serves as a tool for students to fine-tune their work based on personalized parameters and can be applied in the classroom setting.\nOrion North started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nPersonalized Recommendations, Powered by Smart Data\n\n\n\n\n\nOur prompt was to create a content ranking engine for Collide, a community-driven knowledge sharing platform for the energy industry. We were given raw posts, comments, fake user profiles, and we built a website with a recommendation engine for users and posts. It uses semantic searching with a vector database to find similarities among users and their content of interest.\nUsing these technologies:\nWe came up with an algorithm that collects all data related to each user. Their posts, likes, dislikes, and comments are all vectorized, weighted, and then semantic searched against all posts.\nSame as the Hackathon, collecting as many unbiased ideas as possible to expand on creativity and come up with great additions for Collide is a great way to move forward.\nCollide Hackathon\nFront-end development using React + SCSS.\nZahra Bukhari started this project \u2014 5 days ago\nLeave feedback in the comments!\nDelrojo Restrepo \u00b7 3 days ago\nFelicitaciones!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nA minimal, beautiful, inspiring, study app to help you focus and complete your daily homework tasks. Synced through devices with customizable themes. Designed for maximum productivity.\nOur objective is to destroy popular notes app, Notion. We feel that its paid features and difficulty to customize makes it inferior to our free, highly customizable, themes and colours. There is a style and aesthetic for everyone looking to use Motion!\nMotion is a study app, that contains components that allow users to customize their study experience. Things such as draggable components, pomodoro timers, sticky notes, inspirational quotes, ai integration... Motion has it all. Its database integration allows users to save and sync their information across devices, making it accessible from anywhere.\nReactjs for frontend elements, Tailwind Css for stylistic features, and Firebase's firestore for database.\nWe are relatively new to ReactJs, so learning how to refactor things, generalize components was definitely a curve. The integration of various features such as a quotable api, and other css features was another learning experience.\nVarious themes, collaboration, customizable colours, gif integration, learning how to use react, Gemmini AI integration.\nFullstack development requires a lot of work! Even though not everyone in our group is proficient with coding, we all found purpose in divvying up work; some of us worked on UI inspiration, others on backend and coding, and all together our website is very polished in the end.\nFor motion, we intend to add a couple of leaderboard features with our pomodoro timers. Students will be motivated to study and climb a ranked leaderboard, filled with rewards. We intend to add further UI upgrades as well.\nyvrHacks 2024\nJay Wen posted an update \u2014 5 days ago\nThis is so SIGMA\nLog in or sign up for Devpost to join the conversation.\nDaniel Zhang started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nSubmit your notes and be tested on them!\nWe wanted something that can help us with studying our notes\nIt tests you based on a picture of your textbook notes that you upload\nWe first began researching YouTube tutorials about connecting an OCR Scanner to a server that can convert images to text. We then used pytesseract in our code to read text and recognize embedded text in images. Our goal for this text is to use AI to generate a quiz. During the first half of the session, we realized that it was not feasible to directly make a scanner that could read text. Due to this, we changed our application from pytesseract to OCR API which just allows us to upload a photo of the file instead of opening a scanner. We furthermore incorporated Open AI which directly generates quiz questions once you upload your file. The questions are multiple-choice and directly taken from the text that you uploaded.\nAs beginners, many of us started with little to no coding experience, yet we were able to create something functional and real. A significant highlight of our journey was successfully implementing AI and chatbots into our project. This was not only a challenging and time-consuming endeavor, especially since it was our first attempt, but it also opened our eyes to the potential of AI technology.\nWith Kevin\u2019s invaluable support, we were able to overcome the complexities of AI, learning how to integrate it effectively into our work.\nMoreover, we enhanced our website's design, adding features like boundaries and corners that made it visually appealing. The process of coding, teaching, and uploading AI tools brought us immense joy and satisfaction, reinforcing our belief in the power of technology and collaboration.\nWe learned many things throughout this hackathon. First, we learned how to use APIs and how to obtain their codes. Furthermore, we learned how to use pillow, which was used to organize uploading and using the files, as well as ocr.space api which was used to convert an image file into text. Additionally, we learned how to use\nWe can expand our project by improving the design of the website to upgrade the user experience and also update the AI that makes the questions so it\u2019s more accurate and random while making the questions.\nyvrHacks 2024\nI worked on the back-end parts while my teammates focused more on the front-end. There were many bugs while connecting the APIs but I eventually finished.\nRomi Paridel started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nStreamline your nutrition and boost your progress. Find the food that gets you to your goals with our intuitive app.\n\nRestaurant List\n\n\n\n\n\n\n\nWe were inspired by our own struggles to meet our nutritional goals while eating tasty food from our favorite restaurants. We realized there are many people in our shoes who would extremely enjoy a tool that found us appealing meals with our specified nutritional needs.\nCreate an account and sign in with our authentication system.\nNext, select the nutritional groups you are looking for from our selection of protein, carbohydrates, fiber, and calories.\nThen, select a location on Google Maps to search for nearby restaurants.\nNow, enjoy a ranked list of restaurants and dishes that fulfil and prioritize your needs.\nBackend: Python Flask\nFrontend: React with Typescript\nUser Authentication: Firebase\nDatabase: PostgreSQL\nPackaging and Deployment: Docker, Amazon Web Services\nRestaurant Locations: Google Places API\nWe are very proud to display our aesthetic front-end and responsive Flask API and endpoints.\nWe learned a lot about user authentication, packaging and deployment, and working with databases.\\n \\n\nThis project still has a lot of room to grow.\nWe need to figure out alternative solutions for hosting.\nWe also want to implement a user history feature that stores the dishes and restaurants that each user has visited on our app.\nsunhacks\nI worked on the backend, created the docker container to host the sql database, and created the CRUD apis to store and retrieve information\nAditya Gupta started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nWe\u2019ve created a program that will count the amount of people in a room, and relay that information back to you. All you need is our program to automate that for you. No more need for manual clickers!\n\nSettings Screen\n\n\n\n\n\nThe idea for HeadCount came from a career fair, where we noticed employees standing outside the event with clickers to track the number of people entering and exiting, ensuring they didn't exceed the maximum occupancy. This manual process seemed inefficient and prone to errors. We realized that with an automated system, this task could be streamlined, eliminating the need for multiple people to manage occupancy manually. By implementing our solution, we save time, effort, and money. For example, assuming there are 5 rooms with 2 people manning each room at $14.35 per hour, our HeadCount program saves $574 per career fair.\nHeadCount is an automated system that counts the number of people in a room and relays that information to a central dashboard. It tracks occupancy levels in real-time, notifying you when the room is near or has reached its maximum occupancy. This ensures compliance with safety regulations while reducing the need for manual counting at entrances.\nWe used a combination of technologies to build HeadCount:\nAs most of us were new to programming, we encountered several challenges:\nDespite the challenges, we successfully built a functional prototype of HeadCount. This was an incredible achievement for the team, especially considering three of our members were participating in a hackathon for the first time. We learned new technologies, collaborated effectively, and overcame unexpected setbacks.\nThroughout this project, we learned:\nIn the future, we plan to:\nsunhacks\nI worked on the backend and the frontend, as well as the documentation.\nsunyentan Tan started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nMelodia automates the generation of instrumental music based on user prompts.\n\n\n\n\n\nAs a musician, I struggled with adding custom instruments, like Tabla, to existing songs. This inspired me to create a tool that automates the process.\nMelodia.ai allows users to upload a song, choose an instrument, and automatically generates beats that blend seamlessly with the original track.\nWe used Python and the Essentia library for tempo detection, beat generation, and merging the new beats with the original song.\nSyncing the generated beats perfectly with the song was difficult and required multiple iterations to get right.\nWe successfully built a tool that automates the complex process of adding custom beats to existing songs.\nWe learned a lot about audio processing, beat synchronization, and the technical aspects of music theory. We also learned a lot about AWS tools and services.\nWe plan to add more instruments, improve beat accuracy, and offer more customization options for users.\nsunhacks\nWorked on Logic and Algorithm\naakashgangji Aakash started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nRampage: Crowdsourcing accessibility for all Rampage helps users crowdsource and verify accessibility features at social spaces using AI-powered image analysis and geolocation to ensure inclusivity.\n\n\n\nRampage was inspired by a simple but important idea: disabled individuals deserve equal access to social spaces. We wanted to create an app where users could easily find accessible establishments like cafes, parks, or restaurants. Often, finding information about accessibility is difficult, so our solution was to create a Waze-like app for accessibility, crowdsourcing data from users about the features of these spaces, including ramps, elevators, and accessible restrooms. Users can also filter places based on the accessibility features they require.\nRampage allows users to upload images and details about the accessibility features of various social spaces. The app processes these images, detects key features, and stores them in a database, helping other users discover inclusive venues.\nWe built the frontend using Swift, SwiftUI, and MapKit for location-based browsing and ease of use. The backend was developed using Flask and Firebase Firestore for real-time data storage and Google Cloud Vision API for image analysis, identifying features like ramps and elevators from user-submitted photos. We also explored integrating Perplexity AI for validating detected features when Google Vision\u2019s accuracy fell short.\nOne major challenge was the limitations of Google Cloud Vision, which struggled to detect essential accessibility features like wheelchair ramps. We also ran into time constraints as first-time hackers, which prevented us from fully integrating the frontend and backend, although both function independently.\nWe are proud of developing a fully functional backend and a responsive front-end interface, despite the time limitations. The merging of user-contributed data and geolocation-based tracking was a huge milestone for us.\nWe learned the importance of flexibility when working with APIs and how to adapt quickly when encountering limitations. We also learned a lot about time management and estimating the scope of a project. We feel like creating a whole app that integrates with AI and has social features was a very ambitious idea for a 24 hour hackathon, especially our very first one. Most importantly, we explored technologies we had not previously worked with and gained a lot of technical learning experience.\nWe plan to refine the accessibility feature detection improving the AI integration and focus on a complete frontend-backend integration. We believe Rampage has the potential to help disabled individuals access social spaces more easily and will continue to develop it into a seamless tool for crowdsourcing accessibility. All in all, we hope to continue working on Rampage after the Hackathon, and possibly use it as my Immersion Vanderbilt project next year!\nVandyHacks XI\nI built the backend using Flask and Firebase. I used the Google Cloud Vision and Google Maps Geocoding APIs from GCP and also leveraged Perplexity AI for image analysis and feature extraction/description.\nValerie Williams started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nInterior Design for All\n\n\n\n\n\n\n\n\n\nMoving into an off-campus apartment sparked our interest in interior design. As we started planning how to set up our new space, we realized how expensive professional interior design services can be. This inspired us to create NTR-AR, an augmented reality app that allows users to design their own rooms by placing virtual furniture and visualizing setups in real time.\nTR-AR enables users to scan their rooms using AR and RoomPlan, upload those scans to the cloud, and easily retrieve them later. It allows users to place virtual furniture in the scanned room, helping them visualize different layouts before committing to any purchases or changes.\nWe built NTR-AR using SwiftUI and integrated Apple's ARKit frameworks for augmented reality functionality. The backend is powered by AWS, using S3 for storage and Lambda for handling room scans. Terraform was used for infrastructure as code to automate the AWS setup, and API Gateway was implemented to manage network requests.\nOne of the main challenges was ensuring the integration between the AR components and the cloud services. We also faced hurdles with RoomPlan, particularly with optimizing scan accuracy as neither of us had a working Lidar sensor on our phones and uploading large scan files to S3.\nWe\u2019re proud of successfully creating a functioning AR app connected to AWS with terraform.\nWe gained deeper knowledge in augmented reality, cloud computing, and infrastructure automation with Terraform. We also learned valuable lessons in managing complex app architectures that integrate AR with cloud services.\nWe plan to expand the app by allowing users to customize furniture and layouts, improve room scan accuracy, and provide more advanced sharing and collaboration features. We also have plans to try and help people create good feng shui in their rooms/apartments/offices.\nVandyHacks XI\nNafees-ul Haque started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\n300 bikes are stolen from ASU every semester. We solve the problem by integrating a lock within the bike stands at ASU and enabling students to unlock it via a QR code without needing to carry a lock.\nMe and my friends have had our bikes stolen from the ASU campus and know a lot of people whose bikes have been stolen from the ASU campus.\nIt provides a one-stop solution for locking bikes on campus by integrating QR-enabled locks integrated within bike stands.\nHardware: We built it using Arduino 101 and a stepper motor for the hardware and scrap material for building a prototype (Proof of Concept). Software: We used EC2 for hosting our website, SQS for inter-process communication, and DynanoBD for credential management and storing user data.\nDifficulty in setting up Raspberry pir because of which we had to use Arduino because we could not power up Raspberry Pie\nIntegrating our web app with AWS services.\nHardware-software integration\nFor LockItIUp, talk to the ASU administration and figure out how we can help ASU integrate this solution in all their campuses\nsunhacks\nAryan Khanna started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nBoogieWoogie is your ultimate disco-themed companion! Whether you're feeling funky, groovy, or chill, BoogieWoogie recommends the perfect tunes to match your vibe.\n\n\n\n\n\nInspiration We were inspired by the energy of dance and the nostalgia of the disco era. With AI's growing potential, we wanted to combine the fun of Just Dance-style games with AI to create an engaging, competitive experience. BoogieWoogie was born from a love of music, movement, and the drive to challenge users in a fun, dynamic way.\nWhat it does BoogieWoogie is an AI-powered dance face-off app that lets you challenge an AI in a Just Dance-style showdown. With a groovy disco theme, it tracks your movements via the camera and compares them with the AI\u2019s perfect rhythm, creating an electrifying dance competition. At the end of each round, BoogieWoogie scores your performance and lets you know if you\u2019ve got the moves to outshine the AI.\nHow we built it We built BoogieWoogie using a combination of AI for motion detection and performance analysis, integrated with a camera system to track user movements. The design was heavily inspired by the vibrant, colorful aesthetics of the disco era, and the music is driven by a dynamic playlist of funky tracks to keep the energy alive.\nChallenges we ran into We faced several challenges in getting accurate motion tracking to ensure the AI could fairly assess user movements. Syncing the disco visuals with real-time dance moves also required fine-tuning. Additionally, balancing the AI's performance to be challenging yet beatable was key in maintaining player engagement.\nAccomplishments we are proud of We\u2019re proud of creating an immersive, fun-filled dance experience that brings the disco era to life. Successfully integrating AI to offer a challenging yet enjoyable dance competition is something we\u2019re excited about, and the vibrant design brings a unique charm to the game.\nWhat we learned Throughout the process, we learned a lot about using AI for motion detection and performance evaluation. We also gained experience in designing visually engaging user interfaces that maintain fluid interaction between AI and players in real-time.\nWhat\u2019s next for BoogieWoogie Next, we\u2019re looking to expand BoogieWoogie into other areas of movement-based interactions, such as creating a gym repetitions counter to help track exercise routines. We also want to explore multiplayer face-offs, allowing users to challenge each other while still competing against the AI.\nGirlHacks 2024\nI worked on the front end and in the integration of various modules in the project.\nWorked exclusively on the frontend of the project which includes animations and designs using python thru streamlit framework . Also developed this project on a virtual environment to avoid clashes and to manage dependencies\nMurali Sai started this project \u2014 6 days ago\nLeave feedback in the comments!\nSwathi Reddy \u00b7 4 days ago\nLove the concept! It really paid homage to the theme of the hackathon! It would also be great if you could implement this tech for gym instructions, to help you maintain the right posture and other such use cases. Great work!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nPlzFix.ai is an all-in-one AI platform designed to provide devs with tailored and data-driven insights on how to optimize their code.\n\n\n\n\n\n\n\nCode review can be intimidating to new programmers, which can reinforce bad coding habits or scare them off from contributing to open source repositories (We\u2019ve probably all had at least one of those times where you did not contribute due to being scared of getting roasted alive on the internet).\nThe goal of our project is to use AI to help devs visualize, understand, and optimize their projects. By automatically collecting profiling data and integrating generative AI tools, we can provide tailored feedback on not just runtime and space/complexity, but also styling, and structure, empowering all developers regardless of their experience to generate higher quality and more efficient code.\nThe user uploads their C program to our platform. We build and compile the code, and pass it to our profiling tools, which outputs statistics on CPU usage and runtime. We then take this data, along with the original code, and pass it to our LLM agent pipeline, which then outputs suggestions for improvements for each function alongside a code snippet with the suggestions implemented.\nWe use React and Javascript for our frontend, and Python, BAML, Docker, and Bash, and the OpenAI API for our backend. When the user uploads their code, we run it in a containerized gcc environment using Docker, which allows us to run our profiler and get the relevant statistics. We then take this information, and plug it into our LLM agent pipeline, implemented using BAML. The pipeline first gets us the function signatures with each function's content. This then passed into our optimization module, which suggests optimizations for each function and returns an optimized code snippet.\nSeveral of our team members were sick, but we persevered nonetheless. We had some trouble implementing gcc environment. We also had to play around quite a bit with the context and prompting for our OpenAI calls.\nCreating a working LLM agent pipeline with BAML, which we were completely new to, and getting the UI to work properly. We especially like our function diagramming feature.\nWe learned how to use BAML to create an LLM agent pipeline, and how to integrate it with React.\nWe plan to make our visualizations for program flow better, and to integrate our platform directly into the code editor via a VS Code Extension.\nMHacks 2024\nfullstack + llm agent\nBAML LLM Pipelines\nFull stack and UI\nAdvait Patel started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nThe Bloomberg Terminal of Prediction Markets\nSystem Architecture\nOur project is inspired by the intricate relationship between news and market behavior. We observed how publicly traded companies react to news events and how those reactions can be visualized in their stock charts. This led us to explore a more structured approach to quantifying these effects through prediction markets.\nThe project utilizes prediction markets to quantify the impact of news on real-world events. By linking news stories to market predictions, we aim to provide a clearer understanding of how news influences public perception and market behavior. Our platform allows users to forecast event outcomes based on current news, offering insights into the potential consequences of media narratives.\nWe developed the platform using React, LLMs, Python, Flask, VectorDB, Cosine Similarity, Semantics, NLP. We integrated news APIs to fetch real-time news stories and used data analysis tools to evaluate the relationship between news events and market movements. The user interface was designed to be intuitive, allowing users to easily navigate between news articles and their corresponding prediction markets.\nData Integration: Merging data from different news sources with prediction market data posed significant challenges, particularly in ensuring consistency and accuracy. User Engagement: Encouraging users to actively participate in the prediction markets was difficult, especially in a crowded market with many options. Fake News Identification: Developing algorithms to detect and classify fake news without bias was a complex task.\nSuccessfully binding news stories to prediction markets, providing users with actionable insights. Creating a user-friendly interface that simplifies the process of making predictions based on news events. Establishing a framework for real-time analysis of news impact on market predictions.\nThe importance of accurate data sourcing and verification in developing a reliable platform. User feedback is invaluable for refining features and improving engagement. The dynamic nature of news and its unpredictable effects on markets requires continuous monitoring and adaptation.\nExpand our data sources to include international news and various market sectors. Implement machine learning algorithms to improve the detection of fake news and its potential impact on predictions. Enhance user engagement through gamification features and community-driven prediction challenges.\nMHacks 2024\nI worked on building the embeddings api for the back end to match news headlines to tradeable markets.\nJoseph Cook started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nShrinking Size, Amplifying Brilliance\n\nLanding\n\n\n\n\n\nI was initially designing an architecture for detecting American sign language. While making a U-net for segmentation, it struck me how the data was being pushed through a bottleneck to extract the most useful information. This ignited my curiosity as it struck me that I could use this same principle to compress information. Consequently, I started researching AI compression and developed my own model.\nThe model draws out the most crucial information about an image, which reduces the size of the data by a whopping 93%. In some scenarios, it can even compete with JPEG.\nI used PyTorch to build a Generative Adversarial Network. The discriminator is a convolutional network. Meanwhile, the generator is an Autoencoder and Resent combination. The output generated by the first half of the Autoencoder is the compressed latent space, and then the other half of the Autoencoder rebuilds the image from that data. The discriminator is used to train the Autoencoder as it is superior to losses like MSE.\nComputing resources were scarce. After using up my Intel credits, I pivoted to using Kaggle, but it was a lot slower.\nI am proud that I developed a compression model in 24 hours that can reduce 93% of the data size.\nMost ML models I have built in the past have relied on a static loss function. This was my first GAN, and I learned a lot about the clever way that it uses the discriminator as its loss function. Furthermore, as I tried, failed, and tried again to optimize the model, I learned about the deep intricacies of a GAN network, and many of the things that can help and hurt its training. Also, I also learned a lot about resents and how they can aid in image processing in comparison to CNNs. Lastly, I developed a website to showcase this model, and during that I learned three.js, which enabled me to unlock another realm of design.\nPixzip can already compete with JPEG in certain areas, and my goal in the coming months will be to close the gap between Pixzip and JPEG in the other areas. Furthermore, I will be launching a beta API for access to Pixzip compression.\nMHacks 2024\nsolo\nMalhar Khisty posted an update \u2014 5 days ago\nPlease use headphones when watching the video. My microphone malfunctioned.\nLog in or sign up for Devpost to join the conversation.\nMalhar Khisty started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nEmpowering Communities with Data-Driven Solar Solutions.\n\n\n\n\n\n\n\n\n\nThe inspiration for our project was the recent changes in the climate which have been getting more extreme over the past few years. the need for renewable energy where it could be effectively be applied has increased exponentially.\nA platform designed to help NGOs, governments, and energy planners identify optimal locations for solar power installations in areas with high . The app leverages satellite data and real-time community input to map regions with high solar energy potential, facilitating informed decision-making in the planning and development of solar power grids.\nWe built through a variety of programming languages in which the Next js worked as the full stack application by handling the backend and the frontend while the python model runs the most important machine learning model that finds the solar potential areas.\n1)Troubleshooting API errors 2)Transferring data between python and Nextjs files 3)Incorporating a LLM and OpenAPi model 4)Utilizing MATLAB for image visualization\n1)Created a model which takes a user entered value and outputs three locations suitable for setting up solar power 2)Incorporating multiple API\u2019s into one application 3)Fetching data between python and Nextjs through Flask\n1) we learnt about the use of flask 2) how does the MATLAB software work\nExpanding to Other Renewable Sources: In the long term, we\u2019re looking to go beyond just solar data. We plan to integrate wind and hydro power potential into SolarVista, so the platform becomes a complete tool for renewable energy planning. This will help communities and planners choose the best energy source based on what\u2019s most abundant in their area. Developing a Mobile App: We also want to make SolarVista more accessible for local communities by developing a mobile app. This would allow users to easily provide on-the-ground data, even in areas with limited internet access. It\u2019s a way to make sure that everyone can contribute, no matter where they are.\nMHacks 2024\nzainsuch19 Suchedina started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nQuickOTP is a Chrome extension that simplifies login by instantly extracting OTPs from your email for easy one-click copying, saving time and effort\n\n\n\nIn today's digital age, we spend a significant amount of our time in web browsers, whether for work, socializing, or online shopping. It\u2019s common to encounter login processes that require one-time passwords (OTPs) sent to our email, which can disrupt the flow of our online activities.\nThis realization sparked our inspiration to create an OTP filler. We noticed how cumbersome it can be to switch back and forth between tabs to retrieve these OTPs, leading to frustration and wasted time. We aimed to streamline this experience by developing a tool that would allow users to seamlessly access their OTPs without the hassle of tab-switching. By creating QuickOTP, we sought to enhance the efficiency of online logins and provide a smoother experience for users navigating the web.\nThis idea not only addresses a common pain point but also reflects our commitment to improving the overall user experience in an increasingly digital world.\nQuickOTP is a Chrome extension that simplifies the authentication process by minimizing user interaction. When you log in to a website and request an OTP (One-Time Password), our extension extracts the OTP from your newly received Gmail email and displays it instantly for you to copy with just a single click.\nNo more switching tabs, refreshing your inbox, or manually typing codes. QuickOTP does it all for you, seamlessly and securely.\nGoogle Authentication: The extension authenticates your Google account and monitors a custom label (e.g., \"OTP\") in Gmail for relevant emails.\nReal-Time Email Monitoring: Google Pub/Sub tracks changes in the labeled emails and sends a push notification to a webhook(backend server) when a new email arrives and push the email data(historyid) to websocket.\nInstant Notifications via WebSockets: Chrome extension establish connection with websocket and recieves email history id instantly.\n### ---> client side\nEmail Retrieval & AI Processing: The extension pulls the new email content, then sends it to an AI model (Gemini/OpenAI) to extract OTPs or verification links.\nPopup & Autofill: The extracted OTP or link is displayed in a popup. OTPs are automatically filled, and verification links can be opened in a new tab.\nClient-Side Privacy: Google auth tokens are handled entirely in the browser, ensuring they are not shared with the backend.\nEmail Filtering: Ensuring only relevant emails with OTPs were accessed without breaching user privacy was a significant challenge. We designed a filtering system that only monitors incoming OTP emails. Efficient OTP Extraction: Extracting the OTP from various email formats was tricky due to different layouts and styles.\nCreated a system that reduces manual user input to just a couple clicks. Implemented secure integration with Gmail API that respects user privacy. Successfully handled multiple formats of OTP emails.\nIntegration with external APIs like Gmail API requires careful handling of permissions and scopes to avoid privacy concerns. Designing a Chrome extension that interacts with real-time data involves understanding how background scripts and content scripts communicate within Chrome\u2019s architecture.\nAdd support for OTP extraction from multiple email providers. Provide integration with popular password managers for automatic OTP filling.\nVandyHacks XI\nSijan Mainali started this project \u2014 6 days ago\nLeave feedback in the comments!\nAbishek Bhatta \u00b7 5 days ago\nGreat implementation. Loved it..\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nThe worst part of an escape room is that you can only solve it once. With EscapeMate, you can play through limitless new escape rooms without ever repeating a challenge!\n\nEscapeMate logo\n\n\n\n\n\n\n\n\n\nHave you ever been to an escape room and felt disappointed by the lack of immersion? Have you ever found yourself stuck on a puzzle without any hints? With EscapeMate, those days are over!\nEscape rooms are often a one-time experience. Once you solve a room, replaying it loses its thrill. With EscapeMate, escape room managers can modularize their stories and puzzles based on available resources. Input a few parameters (theme, physical requirements, number of players, time limit, difficulty), and watch as the tool generates a tailored story. Utilizing Groq AI acceleration servers for instant output and BoundaryML for seamless integration, EscapeMate transforms repetitive experiences into fresh adventures every time.\nOnce you've crafted a new scenario, you need a way to guide players through the experience. EscapeMate reads the scenario and navigates players through the escape challenge. With Cartesia AI's natural speech processing, players can interact with EscapeMate as they would with a human tour guide, asking questions and receiving personalized guidance. The device also features sensors and tools that enhance puzzle possibilities, allowing it to connect to puzzles, track progress, and adapt to new scenarios. EscapeMate ensures that every escape room experience is unique and engaging.\nWith EscapeMate, immerse your players in unforgettable escape room adventures!\nMHacks 2024\nTanay Sharma started this project \u2014 6 days ago\nLeave feedback in the comments!\nParin Raizada \u00b7 5 days ago\ngoats\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nIntroducing CaliCo AI, effortlessly curating Pitt events to enhance your college experience through the use of the Gemini Platform.\nAs Pitt students, we are given to a lot of different opportunities to explore campus and the surrounding area. However, with so many things to do, it can be overwhelming to pick out events that are specific to personal interests. We wanted to create something to help students capitalize on all of the vibrant events happening in our great city.\nUtilizing the Gemini platform, we give students the opportunity to use their interests and personalities to be able to filter out relevant events in the area. We also give them the ability to easily integrate these events onto their calendars so they don't forget about their plans and can schedule around these events.\nWe started with a python based web parser to collect ongoing and future Pitt events. We then exported these events in a JSON format to our web API. Our web API calls on the google breadboard workflow which is set up to take this list of events in addition to the students interests/characteristics and be able to return id values of different events. We are able to then able to use these id values to visualize the different relevant events on our website. We then allow students to be able to add events to a list and export as an .ics file.\nWhen we were brainstorming, we were not satisfied with the scope of our issue and thus we had to pivot during the event. We also were brand new to the google breadboard interface which meant there was a steep learning curve to be able to effectively integrate this new technology into our project.\nWe were able to get the google breadboard workflow working consistently. We have also been able to create our own API and webpage to aggregate together all of these different elements that we have been working on.\nEveryone came in with their specific specialties/languages that they were comfortable with, however throughout the competition, everyone was exposed to a new technology platform or a new language to be able to make this project a success. We also learned that being able to pivot is a crucial skill to have, especially in fast-paced development.\nWe would like to have a full calendar functionality on the webpage so that students can see whether they have preexisting commitments and plan accordingly. This can also be expanded with different plugins to be able to track tasks and also be able to have events for the greater Pittsburgh area.\nSteelHacks XI\n:)\nAragya Goyal started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nESPER makes the Internet a safer place for those with epilepsy by dynamically adjusting the brightness of videos on the web to make rapid changes more gradual.\nThe ESPER logo :)\nESPER dynamically processes image frames from video HTML elements and analyzes their perceived luminosity. On a frame-by-frame basis, we adjust the brightness of the frame using CSS, ensuring that the change in perceived luminosity from frame to frame never exceeds a defined threshold. This helps mitigate rapid changes in brightness like flashes or strobing that may incur epileptic seizure.\nAdditionally, we collect data on the characteristics of different videos the user views with the extension, and characteristics about the luminosity throughout the video, and upload them to a database. This data can be analyzed in an interactive dashboard to search for trends that may predict whether a given video would incur epileptic seizures.\nOne of the biggest hurdles was choosing to create a Chrome extension instead of a Python script with OpenCV. To maintain accessibility and usability across all parts of the internet without needing to download videos, JavaScript was the preferred choice. However, this came with its own complexities, particularly around YouTube\u2019s strict security policies. Our initial approach utilized iframes, which are often cross-origin, meaning they are served from a different domain (such as youtube.com) than the host page. This raised significant security challenges, like CORS (Cross-Origin Resource Sharing) and Content Security Policy (CSP), which limit interaction with iframe content.\nDue to these policies, we couldn't directly access or manipulate elements within the YouTube iframe, including the video player controls, individual frames, or the DOM itself. Unfortunately, this discovery came late in the process, after a significant portion of our frontend had been built on this method, leading to a major pivot. Beyond this, we also faced challenges in ensuring that luminosity adjustments were fast enough to stay in sync with video playback, and in finding the optimal threshold for luminosity changes to provide a smooth viewing experience as the math is rather complicated.\nShellHacks2024\nI worked on the front end collaboratively with the luminosity logic and a bit of the back end with the API for metadata extraction. First time touching any HTML or Java Script, but great learning experience with amazing support from my teammates.\nI developed the luminosity adjustment formula, the backend, and the database schema, and I contributed to the extension implementation and Snowflake setup. I also deployed the backend.\nI worked on extracting and formatting the metadata from videos for the chrome extension as well as assisted in implementing the luminosity formula to mesh the videos brightness.\nJoshua Sheldon started this project \u2014 6 days ago\nLeave feedback in the comments!\nholmgrengary \u00b7 5 days ago\nNice work!\nKristian Correa \u00b7 4 days ago\nProject is super unique! Anyway yall can make a release on your github. I want to use this extension.\nGabriel Ramirez Perez \u00b7 2 days ago\nThis is so sick, what a cool project!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nThe WiLi Watch is an innovative, interactive wearable wristband designed to enable voice-activated control over various smart home functions, particularly benefiting movement-impaired individuals.\n\n\n\n\n\nIn a world increasingly reliant on technology, the ability to interact with our surroundings effortlessly is essential, especially for individuals with mobility challenges. Enter the WiLi Watch, a wearable wristband powered by Free WiLi and Groq aimed to empower users with independence.\nThe journey began with a vision: to create a device that could simplify and enhance daily living for individuals with movement impairments. Our team recognized the need for an intuitive interface that would allow users to control their smart home environment without the barriers typically posed by traditional devices. By harnessing the capabilities of the watch's built-in infrared (IR) transmitters, we sought to bridge the gap between technology and accessibility.\nThe user wears an embedded wristband device that can be used to control and interact with their environment. Here is the specific tasks that can be performed wirelessly from your wrist:\nThis project relies on the wireless IR communication between a Free WiLi device and a smart home \"hub\" consisting of an Orange Pi 5 and Arduino Nano that control other peripherals such as a camera, door \"locking\" system, IR receiver module, speaker, microphone, and external monitor. The frameworks we used for development are Groq for easy LLM use (Whisper for speech to text and Llama for natural language reasoning) and Cartesia for speech-text interfacing, all developed on a python virtual environment. The project is also modular to be used with Intel's IDC if private LLM generation is desired (we setup a flask server serving llama.cpp inferencing on a compute instance but ran out of credits)\nIncrease in functionalities available on the watch as well as real world implementation (metal instead of cardboard). A few functionalities that we came up with that could be expanded on in further development are as follows:\nMHacks 2024\nEthan McKean started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAI-driven solution designed to detect and prevent fraudulent financial transactions in real time. With cutting-edge machine learning models, it ensures secure and reliable fraud detection.\nFraudGuard\nI was inspired to create FraudGuard because financial institutions, especially banks, are losing billions of dollars to fraud annually. Newly released Federal Trade Commission data shows that consumers reported losing more than $10 billion to fraud in 2023, a 14% increase over 2022 losses. It\u2019s crucial to leverage AI to prevent fraud and provide a secure platform for customers, reducing financial risks.\nFraudGuard is an AI-powered fraud detection system that uses advanced machine learning algorithms and data analysis to identify outliers and detect potential financial fraud in transactions.\nWe built FraudGuard by employing advanced data analysis techniques and three machine learning models: Logistic Regression, Decision Trees, and MLP (Multi-Layer Perceptron). These models work together to detect anomalies and fraudulent activities in financial transactions.\nOne of the main challenges was the lack of access to actual financial transaction data. With real-world data, we could better understand patterns of fraud and design more accurate algorithms for fraud detection.\nWe are proud of successfully building a robust fraud detection model and, more importantly, of our team's collective learning journey. Some of my group members were new to machine learning, and seeing them develop skills and contribute to such a meaningful project was rewarding.\nWe learned a great deal about AI, machine learning, and fraud detection processes. We also learned about the importance of real-world data for training models and how critical collaboration is when working on complex problems that have the potential to impact people's lives.\nNext, we aim to secure internships where we can work with industry professionals, improve our model further, and deploy it in real-world environments. By refining the algorithm with real data, we can enhance its accuracy and effectiveness in fraud prevention.\nShellHacks2024\nAli Muhammad started this project \u2014 6 days ago\nLeave feedback in the comments!\nAli Muhammad \u00b7 5 days ago\nRebeca Serralta is in our team!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nHelp Commander Coco chase after his evolutionary predator the Kitsune.\n\n\n\n\n\n\n\n\n\nThe inspiration behind creating Commander Coco's Stellar Domination, a 2D endless arcade shooter, stems from a combination of personal elements and creative vision. The concept of Commander Coco, a dictator bunny, was influenced by our team lead\u2019s pet bunny. We wanted to give this cute yet seemingly innocent character a twist, so we imagined him locked in an epic struggle against his evolutionary predator, the kitsune, which brought an exciting narrative contrast.\nFrom the start, we aimed to create something that wasn\u2019t just fun but impactful. Our goal was to immerse players in a quirky yet intense world, offering a unique design and aesthetic that sets Commander Coco apart. We combined humor and challenge, reflecting both the high stakes of this dictator bunny's mission and the lighthearted feel of an arcade shooter.\nReplay ability was a key focus for us\u2014we\u2019ve always been drawn to challenging games that push players to improve with each run. By making the game endlessly replay able with increasing difficulty, we aimed to capture that sense of reward that comes with overcoming progressively tougher obstacles. We wanted to create something that balances fun, challenge, and uniqueness, making sure players have a reason to keep coming back for more.\nCommander Coco's Stellar Domination was built using C# and Unity, tools that allowed us to implement fast-paced gameplay and create a visually engaging 2D arcade shooter. Unity\u2019s physics and animation systems were instrumental in bringing our vision to life, but they also came with unique challenges that we had to overcome as a team.\nOne of the most challenging aspects was designing an algorithm for generating random enemy spawns that progressively increase in difficulty. We wanted to ensure that as players advanced, they would face faster projectiles without making the game feel unfair. Balancing randomness with structured difficulty progression required a lot of tweaking, especially during a fast-paced event like a hackathon. We relied heavily on C# to script spawn behavior, dynamically adjusting the rate and difficulty curve as the player survived longer.\nImplementing smooth and accurate collisions was another significant hurdle. We had multiple layers of collision to manage, including:\nAt first, there were issues where the player\u2019s projectiles would collide with their own character or fail to detect hits on enemies properly. Kitsune projectiles would sometimes clip through the player without registering a hit, and in some cases, player and enemy projectiles would interact when they shouldn\u2019t have. We spent a lot of time debugging these interactions, learning how to manage Unity\u2019s physics layers and trigger colliders effectively.\nOur team lead faced the daunting task of creating all the game\u2019s art and animation from scratch during the hackathon. This included designing the main character, Commander Coco, animating enemy movements, and creating dynamic visual effects. Considering the tight timeframe, the sheer amount of work needed for the visual elements was a major challenge. The art had to be not only aesthetically cohesive but also optimized to work smoothly with Unity's animation system, which added to the complexity.\nOne of our team members was working with audio programming for the first time. Implementing background music that responded well in-game presented a steep learning curve. Despite their inexperience, they managed to integrate background ,music which greatly enhanced the overall gameplay experience. Learning Unity\u2019s audio system from scratch during the hackathon was a big achievement.\nCreating Commander Coco's Stellar Domination was an intense yet rewarding process. By leveraging C# and Unity, overcoming technical challenges, and working as a cohesive team, we were able to deliver a game that we're proud of. Every hurdle pushed us to think creatively and work together to solve problems on the fly, especially during the high-pressure environment of the hackathon.\nWe hope you enjoy playing Commander Coco's Stellar Domination as much as we enjoyed creating it!\nShellHacks2024\nRaeus Aranguren-Viegas started this project \u2014 6 days ago\nLeave feedback in the comments!\nAmanda Delgado \u00b7 5 days ago\nVery proud\nDavid Ulloa \u00b7 4 days ago\nCongrats guys!! We were blown away by the polish on the project - especially as first time hackers. You should be really, really proud :)\nJin Carballosa \u00b7 4 days ago\nHuge. Congratulations. Cutest game award\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nMediNform is a software powered by Google Gemini AI that can help bridge the information gap between medical physicians and patients.\nWe were inspired to create MediNform because of how anxiety inducing being sick and the healthcare industry can be. We noticed there was sometimes a divide between physicians and patients and we wanted to help bridge the information gap.\nMediNform leverages speech-to-text technology to capture live doctor-patient interactions, delivering real-time feedback and relevant information tailored to each consultation. Utilizing Google's Gemini AI, MediNform identifies diseases, injuries, medications, procedures, and treatments, offering patients additional context and support. This functionality is designed to ease the anxiety of patients who may feel overwhelmed by their medical situations.\nIn addition to providing reassurance, MediNform serves as an essential accessibility tool for individuals with communication challenges, such as anxiety, hearing loss and muteness. The platform adjusts its explanations based on the patient\u2019s age, ensuring that younger patients receive simplified, easy-to-understand information, while older patients benefit from more detailed, technical descriptions. This adaptability promotes a better understanding of health issues, fostering a more comfortable and informed experience for all patients.\nWhile just a demo, our team hopes MediNform can spark innovation in the way we approch medical consultations. We hope this demo can showcase how the power of AI can be used to help patients communicate with healthcare professionals. Being sick is stressful enough, so we want to put a focus on taking some of that stress away.\nWe used python to create a speech-to-text buffer that updates live with a conversation. And then uses that text output to look for key words in a Trie that contains thousands of medical terms that a patient may not understand. When a keyword is found, MediNform prompts Google Gemini to provide a description of that word and why it's important. These key words can be anything from diseases to medications to treatments. We also included a simple doctor's view to simulate the demo of a doctor talking to and taking notes about his patient.\nOur biggest challenges arose from having to learn many different API's and imports in just the span of 24 hours. Other issues emerged when linking our front end and back end code due to mismanaged communication.\nWe have a working speech to text that can look up key dictionary words and give a response within a reasonable amount of time for the time we had to program. We created an interactive GUI that uses threading to sync the application live with the real-time conversation/consultation. We are also proud to have finished the project fully with little to no errors!\nCommunication is key. Everyone needs to be on board with our ideas, and our ideas need to be properly expressed. Our biggest issues arose from lack of communication and miscommunication. In the end we overcame adversity and were able to completely integrate the separate parts of out code into a project we are proud to have created.\nIf interest is gained, we would love to further our development for small scale testing within hospitals and doctors offices to see just how much better MediNform can make patient experiences\nSteelHacks XI\nI carried out backend implementation of speech-to-text and Gemini communication as well as helped linked front and back end.\nI carried out the backend implementation of the data structure, file parsing, and the Gemini AI system, as well as assisted with the connection between the front and back end.\nI carried out research for implementation routes and developed the front end for the doctor's display that allows the initialization of the patient's window with interpretation.\nI carried out research for implantation of the AI and speech to text into the front end for the patient window. This allows for user to have content from the AI generated on the screen.\nTheo Zervos started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nMany college students lack the initiative to get started with their own insurance accounts. Our goal is to educate and guide students in a simple and fun way!\nState Farm is a well-known company that has thousands of users throughout the U.S. However, many college students lack the initiative to get started with their own insurance accounts.\nOur goal is to educate and guide students in a simple and fun way!\nWe used a basic template from neocities, by Repth: https://repth.neocities.org/theme. We changed all the elements and put our information and ideas into it.\nIt was difficult changing and adding certain things, especially a quiz embedded into the website.\nWe are proud about how much work was done in just a day!\nSkills such as: troubleshooting, collaboration, fast-paced creation\nWe shall see next year in Shellhacks!\nShellHacks2024\nUsed HTML and CSS to build onto the website template, styled pages with new icons and graphics, condensed information into simple and readable formats.\nProvided information on the company and informed the general public on why you should choose statefarm.\nMarian Mora started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nStreamlining Nurse Documentation: Converting Audio Notes into SOAP(IE) Reports with Seamless Database Integration.\n\n\n\n\n\nMany members in our group have a personal connection to nursing, whether it be a family member in the profession or experiences being hospitalized and watching nurses work. We wanted to make a project that gave back to a profession that has helped so many people. 41% of an average nurse's shift is spent doing paperwork, while only 19% is spent on direct care. Our project aims to decrease the time nurses spend doing paperwork.\nThis project takes in an audio file (audio notes from the nurse) and runs a speech processing algorithm to transcript the audio into text. Then, an LLM auto-formats the text transcription into the SOAP(IE) format. This is a formatting style for patient notes that is seen as a best practice. Each letter stands for a category of information the nurse needs to record. The LLM categorizes the info in the text transcription based on SOAP(IE) and inputs this into a database. Currently, users interact with our project through a webpage.\nWe built the webpage components using HTML/CSS/Javascript for the front-end and Flask and Python as the backend. As for the speech processing and SOAP(IE) information categorization, we used Python libraries and an OpenAI API.\nOne big challenge we ran into was optimizing the prompt for the OpenAI API. We needed to give clear and concise descriptions of what each category in SOAP(IE) was, and making these understandable to the LLM required lots of testing. Another big challenge we ran into was debugging the webpage. Our webpage needed to take in audio files, and we had an issue with the audio button not working.\nWe are proud of achieving our initial vision and building a program that is able to fill out formatted patient notes using audio files. We hope that this project can provide value to nurses around the world.\nWe learned the process of how to transcribe audio and how best to design prompts for OpenAI for specific text outputs. We also learned a lot about front-end/back-end development and the debugging process for designing websites.\nOur next step is to integrate our project into commonly used hospital software. We want our project to be easy to use for nurses, and by integrating it into existing software we can decrease the learning curve for our tool.\nMHacks 2024\nEric Teng started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAt LocalPulse, our mission is to put the power of community insights in the palm of your hand.\n\n\n\n\n\nThe inspiration for LocalPulse came from the desire to foster stronger, more informed, and engaged communities. We recognized a gap in the accessibility of local information - often scattered across various sources or simply not available to residents. By bringing together data on crime, small businesses, real estate trends, and more, we aimed to create a unified platform that allows individuals to feel more connected, safe, and empowered in their neighborhoods. Our goal is to transform communities through transparency, insight, and engagement.\nLocalPulse is an innovative app that allows users to connect with their community by providing instant access to comprehensive data and insights. It offers crime analysis, small business details, and city historical trends, giving users a deeper understanding of their neighborhood. With features like real-time police reports, chatbot FAQs with live city information, live traffic and weather trends, real estate analytics and predictions, social media sentiment analysis, and even live camera feeds with object detection, LocalPulse provides a one-stop solution for staying informed. It also includes a community forum for local discussions, the ability to file reports, custom alerts, and much more\u2014all designed to help residents stay aware, safe, and connected.\nWe built LocalPulse using a diverse range of modern technologies that enabled us to integrate multiple data sources, implement advanced features, and create an engaging user experience. For our backend infrastructure, we used Google Cloud for hosting and data processing, leveraging Google SQL for managing our relational database. Docker was used to containerize our applications, ensuring consistent environments across all stages of development.\nThe data analysis and machine learning aspects were powered by pandas, NumPy, and PyTorch, allowing us to handle real-time data analysis for crime, real estate, and traffic predictions. We also used scikit-learn for building models that analyze historical data and predict trends, while PyDantic helped us maintain robust data validation.\nThe front end was built using Streamlit to provide an interactive user interface, supported by Plotly and Matplotlib for visualizing trends and insights in an accessible manner. Additionally, natural-language-processing features were integrated using TextBlob and Google's Generative AI to provide users with chatbot responses and social media sentiment analysis. Deep Translator was used to add multilingual support to reach a wider audience.\nFor live camera feeds with object detection, we utilized computer vision techniques, integrating Google Maps for geolocation tracking and GIS for spatial analysis. Langchain and Claude powered the chatbot, providing an intelligent conversational interface for users. We employed Smarthub for community forum management and API integrations to connect with various local data sources effectively.\nThe system design was visualized and planned using Lucidchart to ensure smooth collaboration among team members, and we used PostgreSQL as our primary database for managing community posts and historical data.\nOne of the biggest challenges we faced was consolidating data from various disparate sources while ensuring its accuracy and timeliness. Integrating real-time data for crime reports, traffic trends, and social media posed technical challenges in maintaining synchronization and preventing data overload. Another challenge was ensuring the security and privacy of user information, especially when dealing with sensitive data like crime reports and user-generated posts. We also encountered some difficulties in optimizing our object detection feature to work efficiently on live video streams without significant latency.\nWe're incredibly proud of the all-in-one nature of LocalPulse and how we managed to bring together so many different aspects of community life in a single platform. Successfully integrating real-time crime analysis, traffic data, and live camera feeds was a major technical achievement. We're also proud of creating a user-friendly and visually appealing interface that makes complex data easily accessible and understandable for everyone. Lastly, developing a chatbot that provides live, up-to-date information about the city was a challenging but rewarding feature that we feel will make a real difference for users.\nThroughout the development of LocalPulse, we learned a great deal about working with real-time data and integrating multiple APIs into a cohesive system. Collaboration was key, and we learned a lot about managing team responsibilities and working under tight hackathon time constraints, which pushed us to iterate quickly and make effective decisions.\nMoving forward, we want to expand LocalPulse by partnering with local municipalities, police departments, and community organizations to further enhance the accuracy and depth of our data. We plan to improve the machine learning models for even better predictive capabilities, particularly for real estate trends and crime pattern analysis. We also want to add more community-centric features, such as localized event announcements, volunteer opportunities, and emergency response integration. Finally, we aim to launch LocalPulse in more cities, tailoring it to meet the specific needs and dynamics of each community, making it a truly indispensable resource for neighborhoods everywhere.\nShellHacks2024\nYaroslav Petrashko started this project \u2014 6 days ago\nLeave feedback in the comments!\nJin Carballosa \u00b7 about 5 hours ago\nLit\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nImplement any video with any color with dynamic resolution.\nOur inspiration for this project was to learn and develop our hardware and fpga skills.\nDisplay short videos at 96x54 resolution with a maximum of 14 frames and 7.5 frames per second.\nWe worked towards the middle. One group worked on the Verilog side while another worked on the Python side. The Verilog side was created with an inferred rom storing each frame and a Verilog file reading each frame one by one on every clock cycle. The Python side worked on a system to convert moving images (ie videos) into a format that can be easily interpreted by Verilog.\nWe ran into challenges for both the Verilog side and the Python side. For Verilog, moving from Cb to Cr to give the monitor colour, getting the coordinates for pixels and the size of the screen(with headers incorporated), putting the code for the images into memory (e.g ram.), and debugging the issue of the monitor not displaying frames.\nBeing able to display a gif of 4 frames at a high frame rate, and with consistent colour for each pixel in the frame.\nVerilog coding\nOn the Python side, restricting the colour gamut to 8-bit colour can drastically decrease file sizes and therefore compile time. It would slightly reduce the quality of the image for the upside of supporting more frames or a higher resolution. On the Verilog side, upgrading hardware and improving memory, speed, and efficiency.\nHack the Hill II\nNeel Patel started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nRevolutionizing the campuses' lost and found with smart image recognition.\n\n\n\n\n\n\n\nWe all have experienced the common frustration of losing items on campus and the inefficiency of existing lost-and-found systems. We realized that we needed a faster and more accurate way for students and faculty to find their lost belongings, and we wanted to bring that platform via a simple website that's not only easy to use, but also solves these grueling problems.\nFoundAI vows to solve this problem by utilizing PyTorch image processing/classification to help you and others find what they're looking for. Simply upload a picture of your lost item to FoundAI and it will automatically give you results for similar items that people have found across your campus. Don't have a picture? You can instead effortlessly describe your lost item to our search bar and using object detection through images, our AI can easily match items that resemble the found items.\nWe used React and Javascript for our frontend. For our backend, we used Python, Flask, MongoDB, and AWS S3. In our frontend, we would call REST API calls to interact with our MongoDB cluster to upload lost and found information. We would upload the photo to the AWS S3 bucket and then update the document with the S3 link. For our search functionality, we utilized MongoDB search indexes and PyTorch. For MongoDB search indexes, we would take the user's search query and then evaluate potential matches. For PyTorch, we used a ResNet18 model to evaluate images and match the user's search photo to a potential match. This let the user quickly either find a matching found/lost item.\nWe faced several technical challenges throughout the development of FoundAI. Initially, none of us were familiar with using PyTorch, MongoDB, or hosting images on AWS S3, which meant a steep learning curve as we tried to integrate all these technologies into our system. Another challenge was determining an optimal threshold for the image classification similarity rank in PyTorch. We needed to fine-tune this parameter to ensure the system returned accurate matches for users searching for their lost items. Additionally, while we used the ImageNet database to train our model, we encountered difficulties due to the overwhelming number of irrelevant keywords. We had to filter out unnecessary keywords and keep only the top relevant ones, which improved both the performance and accuracy of our search algorithm. Balancing all of this with the integration of the React frontend added complexity to the project.\nWe were able to fine-tune the image classification model to accurately rank similar images based on user uploads. Overcoming the challenge of filtering ImageNet data to improve search accuracy was a significant achievement, as it greatly enhanced the user experience. Finally, creating a scalable and user-friendly platform that connects lost items to their owners on campus is something we're particularly proud of.\nWe learned how to efficiently manage and process image data using PyTorch, as well as how to store and retrieve large datasets using MongoDB. Working with AWS S3 taught us the ins and outs of hosting and serving images, and building the frontend with React helped us hone our full-stack development skills. Additionally, we gained a deep understanding of how to fine-tune machine learning models for practical applications, and how to optimize search algorithms by filtering out unnecessary keywords.\nIn the future, we plan to incorporate augmented reality (AR) to allow users to visualize where their lost items might be in real-time based on other users' reports. We also want to improve the accuracy and speed of our image classification model and expand the platform to support multiple campuses and institutions.\nMHacks 2024\nParin Raizada started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\na web app to make voter engagement fun and easy\nIt's hard to find one place that provides all the voting related resources you need without searching the internet looking through different links and trying to find the ones that are relevant to where you live. We wanted to make a website where someone who knows nothing about the voting process can easily get started without jumping through hoops.\nThe user inserts their personal and location related information, which get stored in an SQL database, and the website automatically checks if the user is registered in their state. If not, it provides resources for registering, finding polling locations, voting absentee, and tracking your ballot - each of these are tailored to the user's location. It also show the user a list of the upcoming elections in their area with a countdown. Clicking on the election shows you information about the candidates and how they differ from each other in terms of policy. You can also win badges by successfully completing trivia games!\nWe used HTML/CSS and Javascript with a Flask framework in python. We used the Google Civic Information API to find information given a user's location. We used selenium to check the user's voting registration through their state's website. We used Gemini API to generate candidate info.\nDeciding between an app and website\nThe automated registration checker\nHow to use the civic info API\nDistributing more badges for participating in events, voting in the elections, etc, and have that be part of your place on a leaderboard. More engaging activities, duolingo inspired.\nSteelHacks XI\nRiya Shah started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nA VR breadboard simulator to introduce circuitry basics.\n\n\n\n\n\n\n\n\n\nSoftware continues to dominate the Engineering job market. After all, CSE + LSA CS took up nearly a third of all graduates in the most recent semester. However, we believe that hardware is just, if not more important than software engineering! Through our experiences volunteering with elementary and middle schoolers, we were able to see an appreciation for hardware learning in youth. Through our experiences with VR, both in a hackathon project last year as well as personal projects, we saw an open area for growth in circuit education. Thus, we created V\u00b2/R to provide a more accessible and interactive option to circuitry basics.\nV\u00b2/R is an educational VR breadboard simulator that aims to bring the equipment of a hardware lab to the hands of students. Upon exiting the splash screen, the user will be placed in a simulated environment (that resembles EECS building 215/216 lab). The user is provided with three circuit elements: resistors, wires, and LEDs. For connectivity, a sized down breadboard is available for the student. Eely the IA has lab slides for the student on the projector screen, for which the user can toggle through and read about as they figure out how to build the circuit.\nThe user can place the three elements on nodes of the circuit. Upon completing their circuit, the user can then activate the power supply. The power supply will then simulate the circuit, first ensuring the connections between elements on the circuit and then evaluating the various voltages and currents across each element. This will also evaluate the LED, lighting it up.\nWe utilized Unity, Blender, and Github to complete this project. A Github repository was used as a form of version control between the three members, allowing us to view each others progress as the day went on. Unity formed the backbone of our VR implementation and is stored on the Github repository link below. We also utilized many mesh and .fbx files to simulate the lab environment, as well as recorded audio and imagery for the lab.\nFor distribution of work, a team member worked on evaluation of the circuit, a team member worked on the interactivity with the circuit board as well as overall integration, and a team member worked on 3d modeling and auxiliary implementations.\nOur team encountered some roadblocks through the process of constructing the project. While the learning curve to Unity was far less than Unreal Engine, there still many growing pains throughout implementation. Additionally, due to a rather weak project sharing system on Unity, the team members each had to work on an individual project file. This led to merging issues towards the end of the project that had to be resolved.\nGiven the exploratory nature of Virtual Reality, we are exceptionally proud of the product we produced. Most team members had minimal experience working in Unity as well as C#, and we were able to overcome the boundary with many youtube videos, tutorials, and documentation. Highlights of this project include the connectivity between each element and the nodes of the breadboard as well as the circuit analysis program.\nThrough the making of this project, we got to get more hands-on experience with the Unity XR toolkit. Additionally, the project allowed us to brush up on our circuitry and graphing knowledge, and we got to learn about circuit network analysis and apply a linear network analysis on the circuit components. We created an algorithm that would be able to recognize all of the simple closed loops implemented on the circuit and then the algorithm could print out what the current flowing through the component and resistance would be.\nThe mathematics behind calculating the current and voltage of different circuit components is extremely intricate and complicated. Given the limited timeframe of MHacks, we were forced to leave behind many other core circuit capabilities and strictly limited to DC circuits. In the future, we would like to accommodate AC circuits including elements such as capacitors and inductors as well as op amps. Along with more circuit elements, we would like to have more testing equipment working. This would ideally be an oscilloscope and function generator as well as a voltmeter/ammeter.\nCircuitry is a large topic that has many concepts. Through this project, we\u2019ve shown that circuitry simulated in VR is accessible and can be extremely complex. We hope to inspire more hardware learning in K-12 education and we hope to give more power to the students!\nMHacks 2024\nGrant Wang started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nPawgress makes you Progress\n\n\n\n\n\n\n\n\n\n\n\nWe created a self-improvement app that's different from the rest. Most existing apps fail to adjust to each person's unique knowledge, skills, and progress levels. Motivated by this gap, we set out to change that! By incorporating OpenAI's powerful technology, we\u2019ve designed an app that adapts to you\u2014personalizing your self-growth journey based on your individual needs.\nInspired by the charm of 2D pixel art games, our app turns your progress into a fun, gamified experience. It\u2019s not just about improvement, but also about motivation\u2014because who doesn\u2019t love a good game? Plus, we've added an adorable twist! You\u2019ll have a cute pet companion by your side to help you along the way, making your self-growth journey both productive and calming.\nAfter acquiring some important information from the user to understand their needs, the appliation will make analysis and produce a detailed plan for them in the form of a roadmap. The roadmap shows in each step, how many days of a period that would need, detailed action and point rewards system associated with their account, accumulate growth.\nWe used Python Flask framework to build the backend, Next.js, Tailwind CSS for frontend development, Firebase for data storage, Open AI API for generative plans and any related requests.\nDepending on the settings on Open AI like Token limits, we received different formats of the response which created a bit of trouble when automating the parsing process.\nOur UI/UX is very user friendly and unique, that will attract many users for the clean and intelligent design and help them better engage with the experience as the product supports them along the way to self-improvement.\nTeamwork, stress management, Web App Development, different styles of user experience design\nKeep on improving features to help users make plans for achieving their goals by expanding media types, like voice assistance, generative images.\nHack the Hill II\nSelin Kararmaz posted an update \u2014 4 days ago\nThis was a very fun project to work on, thank you guys for all your work!\nLog in or sign up for Devpost to join the conversation.\nJun Ye posted an update \u2014 4 days ago\npawgrees make you progress!!\nChenhao Wei \u00b7 4 days ago\nwoof woof\nLog in or sign up for Devpost to join the conversation.\nAditya Kandel posted an update \u2014 4 days ago\npawgrees make you progress!!\nChenhao Wei \u00b7 4 days ago\nwoof woof\nLog in or sign up for Devpost to join the conversation.\nChenhao Wei started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nIts like Bumble but we let you Stumble! Stumble is a LeetCode-style dating trainer to help every CS student conquer their greatest fear!\n\nStumble\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe inspiration behind Stumble came from those around us we came to the realization that dating, much like coding, is a skill that can be improved with practice. We wanted to create a platform that merges the structure of a learning tool like LeetCode with real-world dating scenarios, providing users with a space to hone their conversational abilities. By gamifying the dating process, we aim to help people feel more confident and prepared for meaningful connections.\nStumble is a dating trainer app that simulates common dating scenarios for Beginners and advanced users. Users can interact with AI-driven dialogues and receive real-time feedback on their conversational choices. The app helps users practice everything from icebreakers to deeper discussions, allowing them to develop skills that make dating less intimidating and more intuitive. Just like Leetcode Modules are designed with difficulty in mind and every module must pass through 3 test cases.\nWe built Stumble using a combination of front-end and back-end technologies. The AI model that powers the conversation engine was developed using natural language processing (NLP) tools like ElevenLabs and OpenAI, while the user interface was crafted with React for a familiar and intuitive experience.\nOne of the major challenges we faced was fine-tuning the AI to understand and respond naturally in various dating scenarios. Additionally, ensuring user privacy and data security while interacting with the AI model was a significant hurdle.\nWe\u2019re proud of creating a product which can help so many. We believe that Stumble an effective learning tool to help some overcome what could be their greatest hurdle in life, making self-improvement fun and accessible.\nThrough this project, we learned a lot about human-AI interaction and how subtle nuances in communication can have a big impact on user experience. We also gained insight into the challenges of building a product that blends education, entertainment, and personal development.\nWe recognize that we are coming from a very male and heteronormative view looking forward, we plan to introduce more advanced conversation scenarios, tailored to variety of users. We also aim to add features that allow users to track their progress over time and receive more personalized tips for improvement. Expanding Stumble into multiple languages and cultures is another goal, making it a global tool for dating practice and confidence-building.\nThat's Us! Remember its OK to STUMBLE!\nHack the Hill II\nJoseph Liao posted an update \u2014 5 days ago\nWOW i wish i had something like this, I would defiantly have a girlfriend by now!\nLog in or sign up for Devpost to join the conversation.\nEvan Ferreira posted an update \u2014 5 days ago\nThis is such a good project and a revolutionary idea --- I will 100% use this to improve my relationship!!!!!!!\nLog in or sign up for Devpost to join the conversation.\nJoseph Liao started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nDive into intense 3D dance-offs! Use simple gestures to animate a 3D version of yourself performing insane breakdance moves and challenge friends side by side. Show your groove!\n\n\n\n\n\n\n\n\n\nEver looked at yourself in the mirror and thought, \"I've got two left feet, but I'd love to see myself bust out some sick moves\"? That's exactly where BoogieBattle was born! We're a group of self-proclaimed dance floor disasters who dreamed of a world where our digital doppelgangers could groove like pros. Our lack of rhythm became our greatest inspiration, pushing us to create a game where anyone can become a dance legend, regardless of their real-life dancing skills.\nBoogieBattle is a 3D dance battle game that lets players live out their wildest dance fantasies using personalized 3D models of themselves. Here's what makes it groove:\nWe put on our developer dancing shoes and got to work:\nOur journey to create BoogieBattle wasn't all smooth moves. We encountered several challenges along the way:\nDespite the challenges, we achieved several milestones that make us proud:\nBoogieBattle was a crash course in game development and cutting-edge technologies:\nMHacks 2024\nIshaan Arya started this project \u2014 6 days ago\nLeave feedback in the comments!\nDharmil Shah \u00b7 5 days ago\nThis is a good idea, now people who are not that good dancers can also use this to choreograph moves and show their talent using this, not being restricted to actually making moves which are sometimes harder to do. Nice Idea!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nA Hormone Replacement Therapy (HRT) tracking assistant\n\nbutterfly\n\n\n\n\n\n\n\nA Hormone Replacement Therapy (HRT) medication tracking assistant to provide ease-of-mind, tools for self-advocacy, and celebrations of milestones throughout the entire process.\nTrans healthcare is one of the most systemically disregarded and overlooked areas of healthcare. Coupled with the lack of technology aimed to support trans individuals, this leads to a huge discrepancy in trans experiences with healthcare, and HRT specifically. Butterfly aims to be an easy-to-use, discreet application that prioritises not only trans advocacy in healthcare, but equally prioritises celebrating trans joy.\nBuilt on react-native with expo, and figma.\nThis was my first time (trying to) build an app so I couldn't get too much into an actual demo. I've never used react-native before, let alone code in javascript so I definitely spent the majority of the hackathon figuring out a lot of basic things. I also ended up spending (waaaay) too much time on the bottom taskbar and figuring out expo router navigation, but it was fulfilling to have the taskbar design closer to how I wanted it to be!\nI'm glad I got part of it working, and I've discovered loads of resources that I'll be looking into. I've always been daunted by React and app development in general, so I'm glad I managed to get something started. I also was not planning on making a submission, but I ended up choosing a project that matters a lot to me so I wanted to present it, and I think that's a win!\nTechNova 2024\nSoumya Menon started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nLearn ASL in AR through Computer Vision and Hand Tracking\n\n\n\n\n\n\n\nAmerican Sign Language is the primary language of individuals hard-of-hearing in the United States. We were inspired by Meta's hand tracking and our values around accessibility to make ARSL (Augmented Reality Sign Language).\nARSL uses Computer Vision to detect objects through your Quest headset passthrough and translate detected objects into ASL alphabet. Then, hand tracking combined with haptic vibration feedback guides you through spelling the object.\nWe developed an AR application that interfaces with a TCP server to classify everyday objects, aimed at teaching sign language through augmented reality. To enhance user experience, we created a vibrating glove that provides haptic feedback, indicating how close the user's fingers are to the corresponding sign.\nWe have so many ideas for the future of ARSL! Some feasible goals for the near future would be improving gesture detection accuracy, refining object detection at greater distances, and enhancing the user experience with cleaner visuals. An additional, word-focused mode would also be an excellent feature to bolster users' ASL education (which was dramatically improved during the course of the 36 hour project!).\nShellHacks2024\nI constructed the back-end pipeline -- the object detection and identification server and the TCP tunnel and image stream that connected the Quest and the server, including Unity scripts.\nGabriel Ramirez Perez started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAnswer infinite trivia questions with a TikTok inspired scrolling format.\n\n\n\n\n\n\n\n\n\n\n\nTikTok, Instagram Reels, Kahoot.\nSteelHacks XI\nI worked solo this hackathon. Everything you see was built out by me alone.\nCaden Milne started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nFun Led Minigames\n\n\n\nWe love competitive games like Mario Party and the cuteness of small devices like Tamagotchi. We wanted a project that combines the two, which is why the free wili seemed like a great system to use.\nGives users the ability to play four 1v1 games\nWe used two Python libraries. The freewill API to communicate with the free wili and flash our code, and the pygame library to receive and read controller input. We also have a given header file for C++ definitions of useful functions that allow us to configure the free wili board, such as LED toggles and Display images.\nThe free wili had some hardware limitations that prevented us from fully fledging out our implementation as was planned. Primarily, its I/O capabilities are limited, and cannot read controller input through USB-C as a general purpose computer can, so we had to keep the system wired to the computer during play. We also ran into space issues in uploading images to the display, so the UI is not as in depth as we would have liked.\nFor having found out about this device only 24 hours before submission, we are very proud of the fact that we were able to flash the device with our software, turn on LEDs, upload images, and have it function properly with our controller input, despite the limitations we encountered.\nWe learned a lot about how free wili works, how to write to it, and its functionalities. We also learned how to use pygame to process user inputs from a controller, to parse and control them correctly, and also how to manage multiple controllers.\nIdeally, we would love to make the controller input work without running it through a computer for processing. We want the games to be able to be fully run on the free wili itself, with just two controllers as support. We would also like to make the communication wireless, so we are not cluttered with USB-C cables while playing.\nMHacks 2024\nAndi Shaska started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\n\"Master your interviews with AI-powered mock sessions, tailored just for you and the job you are applying to!\u201d\nThe competitive nature of the job market inspired us to create a solution that helps job seekers feel more confident and prepared for interviews. Many candidates struggle with interview anxiety, so we wanted to provide an accessible, AI-driven tool that mimics real interview scenarios and offers personalized feedback. Our goal was to make interview prep not only effective but also interactive and fun.\nPrepify simulates real-time mock interviews tailored to specific job descriptions. Users can upload their resume and job details, select the interview duration, and receive role-specific questions generated by the ChatGPT API. Prepify also provides voice-based interactions, utilizing AWS Transcribe for speech-to-text analysis and AWS Polly for lifelike voice prompts. The platform gives users immediate feedback, helping them refine their responses and improve their performance for real interviews.\nWe developed the backend using Node.js, integrating AWS services such as S3 for storage, Transcribe for speech-to-text conversion, and Polly for generating interview prompts. The frontend was built using React and Material UI, providing an intuitive user interface for uploading resumes, job descriptions, and selecting interview durations. We used the ChatGPT API to dynamically generate interview questions based on the input data, making the experience highly personalized.\nOne of the major challenges we faced was handling real-time voice interactions. Integrating AWS Transcribe and Polly to work seamlessly with the ChatGPT API required careful synchronization to ensure accurate speech recognition and natural-sounding prompts.\nWe're proud of creating a full-stack solution that offers a unique, AI-powered interview experience. Successfully integrating multiple AWS services and building a polished UI in a limited timeframe was a huge accomplishment.\nThis project deepened our understanding of AWS services, API integrations, and how to create a seamless user experience. We learned how to handle real-time voice interactions and speech recognition, as well as how to build an AI-driven platform that provides personalized interview prep. Collaboration and quick problem-solving were key in overcoming technical hurdles.\nsunhacks\n- Developed the UI for Prepify\n- Worked on the setting up web sockets communication\n- Researched techniques to extract text from different file types.\n1. Set up the web socket connection which enabled real time communication.\n2. Integrated the OpenAI API to respond to user inputs.\n3. Some parts of the UI\nI worked on building the Pipeline from obtaining user voice input and transcribing it using AWS Transcribe, passing it to OpenAI API and converting the response from OpenAI to speech using AWS Polly.\nI also helped with some front end work\nI worked on integrating AWS Transcribe to convert user voice input into text, which was then processed by the OpenAI API to generate interview questions and responses. I also handled the integration of AWS Polly to convert the AI-generated text back into speech\nSriranjini Ramesh Vasista started this project \u2014 6 days ago\nLeave feedback in the comments!\nRishi tadavarthi \u00b7 4 days ago\nWow amazing work !!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nOrdering pizza can save your life, trust me.\n\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Canada, a staggering 46.2 million women aged 15 and older have reported experiencing abuse in intimate partner relationships. For many, domestic and sexual violence are harsh realities that women face every day. When these distressing situations escalate to legal confrontations, the unfortunate truth remains that a man's word is worth significantly more. Hence, we built Pizza Studio, a cleverly disguised pizza-ordering platform that looks like your neighborhood Domino's. It helps women collect crucial evidence during dire situations where no one believes her word.\nIntegrating front-end and back-end applications is always tricky, but through error analysis and patience, we were able to figure things out! When we were thinking of our idea, we got excited by the concept of voice triggers and auto-detection. However, as we continued building the project, we realized that it was not feasible and had to adapt to our current circumstances by changing our features.\nNine hours before the hackathon, we thought that there was no way we were going to finish this project. But despite how pessimistic we were, we all decided to stay up and challenge ourselves, leading us to build a product that can help millions of women who face abuse every day.\nEnabling voice triggers so the user can collect evidence from a safe word without manually picking up the phone. We are also looking to incorporate an initial tutorial so users can understand the underlying meaning behind our disguised features.\nTechNova 2024\nIris Mo started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nDo a move with Duo Move! Improve your physical dexterity and strengthen your friendships with our AI powered, motion tracking, person vs person game.\n\n\n\nOur inspiration stemmed from our shared passion as software engineering students for combining technology with physical activity. We noticed that while tech-related hobbies often keep us seated for long periods, we yearn for fun activities that get us moving and physically active. That thought led us to create Duo Move, an interactive game that not only encourages movement but also utilizes cutting-edge AI technology to provide a fun, immersive experience.\nOur project uses a Raspberry Pi with an integrated RPI5-Hailo8L AI chip to capture real-time video from a connected camera. The AI detects players' joints and body parts, which are then used as inputs to control the game.\nWe set up a client-server architecture where the Raspberry Pi processes video data using AI and sends real-time motion information to the game client via a custom WebSocket communication. The Raspberry Pi acts as a local network as well. The server side runs a Flask-based backend that captures the video feed and performs pose estimation, while the client side, built with JavaScript, renders the game and updates based on the received data. We integrated frameworks like gstreamer for streaming and used Flask to manage the data flow.\nOne of the key challenges we faced was getting the live video feed from the Raspberry Pi to render properly in a web browser. The second challenge involved mapping the coordinate system of the real-world camera feed to match the game's coordinate system, ensuring that player movements were accurately represented in the gameplay. It was also challenging to find documentation on the newer or niche technologies we worked with.\nWe're immensely proud of being able to incorporate AI into fun solutions to problems of not getting active while playing with a friend. We're particularly happy about the seamless integration of the joint detections and being able to display it on screen!\nWe learned various things in regards to the tech stack. This project was the first time some of the members used a Raspberry Pi for a project and some frameworks, like gstreamer to make an AI pipeline and Flask, was learned on the fly to ensure us delivering on this software.\nAdding more game modes, adding player profiles and expanding the player base.\nHack the Hill II\nvinisha manek started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nRobust. Safe. Fast. Mound is a decentralized peer-to-peer file-sharing platform that lets you share files, maintaining data integrity across the board.\n\n\n\n\n\n\n\n\n\n\n\nAfter using the uOttawa internet and failing to download a few large files due to terrible connection stability, we decided there must be a better way. On top of all of the data breaches nowadays, we wanted a solution that was fast, secure, and extremely robust.\nMound is a decentralized, peer-to-peer file-sharing platform that focuses on being robust and secure. With limitless network sizes, Mound can be used to share files between your close family, school, or entire city. When a file is uploaded to a Mound network, it is compressed, split into chunks, and each chunk is sent to at least two nodes to guard against loss of data. To stick with our goal of being robust, Mound computes one checksum for each file chunk, and one (larger) checksum for each complete file - that way you know you're getting exactly what you asked for. It also includes failover rules to fallback to a different peer if a certain chunk cannot be downloaded (or was transmitted badly), ensuring your downloads never stop mid-way again. On top of everything, we implemented smart routing and AI-based network instability detection, ensuring you stay protected against loss of connection, unstable peers, and dropped packets.\nMound has two parts: the mound command-line tool, and the Mound user interface. For technical users, the command-line tool can be launched and interacted with using a protocol similar to JSON-RPC. For non-technical users, the user interface exposes all Mound functionality by launching the mound process in the background and wrapping it into an enjoyable experience.\nThe mound command-line tool was built with Rust for its reliability, performance, and low-level networking control. We used the TCP protocol at the lowest level, building our own packet and data transmission protocol on top of it for maximum control.\nThe Mound user interface is built on Electron for its simplicity and compatibility with the Rust command-line tool using Electron's IPC protocol. It's also extremely portable, allowing you to run Mound everywhere you go.\nWhiteboarding a design that was both performant and extensible was very difficult: we had to take into account all three stages of the Ciena challenge in order to avoid re-designing our system multiple times. However, this effort paid off, as we were able to utilize this design and bring it straight to the finish line without many refactorings, performance problems, or glaring errors.\nWe're proud of how far we made it into the Ciena challenge stages: completing all three and adding a few goodies on top. We were able to integrate our own AI model to determine network stability issues, add a compression layer with zlib, and create a novel chunking algorithm to increase throughput - all on top of our original design.\nFor the mound command-line tool, we learned a lot about networking, TCP, various different protocol designs and algorithms, and AI models for regression analysis. For our frontend, we had never used Electron before, so integrating it with the IPC protocol, designing a stream-based interface, and making it all fit together was a difficult task.\nWhen making the mound tool reliable, we learned a lot about checksums, hashing, and failover algorithms and how their use significantly improves the reliability and integrity of data. This allowed us to feel confident that Mound would work under the vast majority of network conditions.\nThe next step for Mound is focused on security and multi-network setups. For security, we're looking to implemented password-protected networks with 2FA to safeguard files against malicious actors. To improve the experience for more advanced users who join multiple networks, we'll implement a \"profile switcher\" to switch between different networks on the fly.\nHack the Hill II\nMatthew Polak started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nData centers consume massive energy, straining grids as clean energy availability fluctuates. We aim to help operators allocate loads based on energy availability to maximize the use of clean energy.\n\n\n\nAfter watching this CNBC video, we became aware of the inadequacies in the US electrical grid, with a ChatGPT query requiring 10 times as much power as a Google search. We wanted to know if there was a way to map specific tasks to servers while attempting to minimize the overall energy footprint. With our backgrounds in Electrical Engineering and Math, we wanted to explore an area that overlaps both our interests.\nGiven weather information at server locations, we built a model that provides a metric for the reliability of renewable energy resources at a specific location. We use this metric to weight our load-balancing algorithm so that it optimizes for renewable energy.\nWe approached the problem of server load distribution as a k-server problem, frequently considered the holy grail of optimality problems, with a weighting to account for a location's renewable energy footprint. For our data inputs, we used two APIs: one from the US Energy Information Administration (EIA) and Open-Meteo, an open-source weather API. We processed this data using JSON and pandas, so the two data sources were aligned and in compatible formats with one another. We addressed data abnormalities and built custom features to improve our model (feature engineering).\nThe model we chose was an LSTM due to its fast inference capability and its ability to handle time-series data well, with its inherent \"forgetfulness\" being beneficial due to seasonal weather changes. We implemented the model in Tensorflow. After producing the model, we inferred the renewable reliability using 3-day weather forecast data as inputs for a range of server locations. These served as the weights for our k-server problem. We chose a randomized heuristic since this problem is NP-hard, but this heuristic is generally considered one of the best pseudo-optimal algorithms for addressing it. Our metric for distance in this algorithm is load, which allows us to assign tasks to servers using our weights to form a probability distribution. Sampling from this distribution provided us with a pseudo-optimal load balancing of tasks among our servers, while also prioritizing each server\u2019s ecological footprint.\nWe chose a Python-Streamlit frontend due to its variety of input and output widgets, which provide a high degree of user interaction. Users input server locations and can generate random tasks to test the model's weighting and observe the load balancing in real time. Servers with high weights are assigned a higher load due to their increased ecological reliability, and these weights converge to a stationary distribution as n (the number of tasks) becomes significantly greater than k (the number of servers).\nWe faced conceptual challenges in understanding the dynamics between electricity suppliers and data centers, trying to balance both parties' renewable energy goals in our model. We encountered data scarcity and cycled through many APIs before finding the right ones for the task. Due to the lack of training data, we prioritized feature engineering to get the most out of our smaller dataset. We also spent time considering which k-server heuristic would best align with the problem. Most material on the k-server problem is theoretical, rather than application-focused, so proceeding without much guidance proved challenging.\nWe chose a nuanced problem space with real-world applications. Being able to leverage classical computer science theory in the k-server approach, along with a more modern LSTM ML architecture, provided a novel solution to a largely unexplored area of sustainability. Although frustrating at times, the outcome was fulfilling as we were able to validate many of our hypotheses.\nWe learned how to mentally overcome development hurdles. We learned how to best utilize the skillsets of our team members, allowing for an efficient division of labor that expedited the development process. We also gained a deeper understanding of the power grid and the field of sustainability, which is often overlooked in CS.\nWe hope the data we generate can facilitate Power Purchase Agreement (PPA) negotiations between power generators and data centers, as data centers and generators face stricter energy quotas. This could reduce overall energy wastage and benefit both parties economically.\nMHacks 2024\nAditya Ashok Sinha started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nVoice to Text Rapid Response LLM to address immediate health issues.\n\n\n\nThe wound tracking app is designed to be a comprehensive tool for patients who are undergoing wound care. Its primary goal is to support patients in their wound care journey and make it easier for them to monitor the healing process. The app aims to improve patient engagement and increase their sense of self-efficacy in wound care, by providing them with easy access to important information about their wounds. This can be incredibly beneficial for patients who are managing multiple wounds, as it provides them with a centralized place to store and review their wound information.\nThe application aims to develop a computer vision system that classifies and diagnoses wounds through image processing and convolutional neural network (CNN) techniques. The system will use digital images of wounds as input and perform image processing to extract features relevant to wound diagnosis. The features will be fed into the CNN model, which will classify the wound into different types and provide a diagnosis of the wound's severity. The model will be trained on a large dataset of wound images and annotations to achieve high accuracy in wound classification and diagnosis.\nThis project will create a mobile application that utilizes a machine learning model, more specifically a convolutional neural network model created by the researcher, to classify different plant diseases based on images. The app should classify field data at a high accuracy in addition to data from the dataset it was trained on.\nThe wound tracking app presents numerous opportunities for future research to advance our understanding of wound care and management. Evaluating the app's effectiveness in the following areas is crucial: Improving wound healing outcomes and reducing complications. Patient satisfaction, engagement, and self-efficacy in wound care. Feasibility and acceptability of the app in various patient populations, including older adults and those with chronic conditions or limited access to healthcare.\nThe WoundAI mobile application is designed to make wound diagnosis fast, accurate, and accessible to medical professionals and patients alike. The app is user-friendly and operates in the following way: Image capture: The user can capture an image of the wound using their mobile device's camera. Image classification: The app processes the captured image, using the trained CNNs, to make predictions about the type of wound present in the image. Results display: The app displays the results of the image classification, including a confidence score for each wound class, to the user.\nIn conclusion, the WoundAI project has successfully demonstrated the feasibility of using computer vision techniques, specifically image processing and convolutional neural networks, for the task of wound classification and diagnosis. The proposed system was able to achieve high accuracy in the classification and diagnosis of wounds through the use of effective feature extraction techniques and the training of a deep learning model. The implementation of the WoundAI system involved the use of popular computer vision libraries such as OpenCV and deep learning frameworks such as TensorFlow. The system was designed with a user-friendly interface, making it accessible for medical professionals to utilize in clinical settings. The results of this project showcase the potential for the use of computer vision and deep learning in the medical field, specifically in the area of wound diagnosis. The success of the WoundAI system highlights the importance of continuing to explore and develop innovative solutions using these technologies.\nIn addition to these key areas, future research could also explore: The impact of the app on communication and collaboration between patients and healthcare providers. The influence of the app on healthcare resource utilization, such as reducing hospitalization rates and overall costs. The app's role in promoting patient education and self-care in wound management. By studying these areas, future research has the potential to further our understanding of the benefits and limitations of the wound tracking app and inform the development of effective digital health solutions for wound care and management.\nHackGT 11: Circus of Inventions\nI contributed to the project by creating the video presentation, assisting with front-end development, and collaborating on the UI/UX design for the AI segmentation feature. Additionally, I played a key role in supporting the backend implementation, ensuring seamless integration of the AI functionality with the user interface.\nankit06ch Chandra started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nA hierarchal multi-agent framework built from scratch. Transforms complex goals into subproblems, that are dynamically assigned to specialized agents, boosting productivity and lowering cost.\nMHacks 2024\nJonathan Lima de Paula started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nSafeSnap is a novel lens that is designed to guide individuals during natural disasters and other emergency situations by leveraging the cutting-edge capabilities of SnapAR Spectacles.\n\n\n\n\n\n\n\n\n\n\n\nWe wanted to think of an idea that would be helpful and functional using snapAR\n3D scan of the room with identifiable objects that has audio and visual cues that tells the user when they are in a safe location. Also equipped with a floating checklist that follows the user.\nUsing lens studio and implementing snap AR.\nNavigating snap AR and implementing complex features like effects and checklist executions.\nGaining knowledge of a tool we didn't know how to use before.\n3D modeling and design\nWant to be able to train safesnap using ML to identity those objects and locations instantly and provide a easy safe location guide.\nHackGT 11: Circus of Inventions\nI worked on coming up with the initial idea and creating the voice activated checklists. I helped format the checklist, integrate it with the voice recognition asset for SnapAR, and make it toggle to either follow the user through the frame or stay in one place.\nYash Alluri started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nProspera helps immigrants boost their FICO score by analyzing income, spending, and dependents. With tips, a personalized score, and our friend Foxy, users can improve their financial habits!\n\nThis is the Prospera home page, where users can integrate their financial standings to receive a score.\n\n\n\n\n\n\n\n\n\nAfter our team experienced firsthand how financial literacy impacted some of our immigrant parents, we wanted to help other immigrant families with limited resources feel empowered to start their financial journey correctly while balancing the challenges of adjusting to a new place. With Prospera, we made it easier by avoiding overwhelming statistics and instead using Foxy, a friendly avatar that visually represents their financial status. By analyzing income, spending, and dependents, Prospera provides personalized scores, actionable tips, and guidance to help users improve their financial habits and boost their FICO scores.\nWith Prospera, we focus on simplicity:\nProspera makes financial literacy accessible and stress-free!\nWe built Prospera using React and Vite for a fast, smooth user experience, with Tailwind CSS for clean design. We utilized OpenAI's API to power Foxy, our chatbot, which scrapes user data like household income, spending, and number of dependents to create a personalized financial plan.\nOur unique scoring system helps low-income immigrants by evaluating:\nIncome Score: Based on household income relative to local poverty and median income. Spending Score: Reflecting how much income is used for expenses, encouraging savings. Household Size Adjustment: Adjusting for financial strain based on family size. As users improve their financial habits, Foxy\u2014our friendly avatar\u2014changes to a happier version, providing a visual reward and motivation to keep improving. This way, users can see their financial progress in a simple, engaging way, without feeling overwhelmed by statistics.\nDuring the development of Prospera, we encountered several challenges that required collaboration and problem-solving.\nMerge Conflicts: One significant hurdle was dealing with merge conflicts, as multiple team members were simultaneously editing the header and financial calculator page. This required careful coordination to resolve overlapping edits and ensure a smooth integration of everyone\u2019s contributions.\nOpenAI Integration: Integrating OpenAI into our code presented difficulties, particularly with accessing the API key. This issue prevented Foxy from successfully scraping user-provided information for personalized financial guidance, necessitating extensive debugging to resolve.\nScoring Mechanism Development: We also faced challenges in creating our own measuring score for financial health. This sparked debates about how to effectively scale specific information and data in our calculations. We had to ensure that our scoring mechanism fairly represented users\u2019 financial situations while remaining relevant and actionable. This involved discussions on normalizing household income, spending, and household size without introducing bias, as well as determining appropriate weights for each component of the score.\nThrough collaboration and persistence, we were able to overcome these challenges and create a robust platform that effectively serves our target audience.\nWe take pride in how we prioritized user experience throughout the development of our financial website, Prospera. We recognize that finances can be stressful and confusing, especially for immigrants navigating a new environment and system. To address this, we created Foxy, a friendly and user-friendly avatar that visually represents users' financial states. Foxy not only helps users understand their current situation but also provides valuable tips and resources to guide them on their financial journey. By focusing on making financial literacy accessible and engaging, we empower users to take control of their finances with confidence and clarity.\nThroughout this project, we learned valuable lessons that enhanced our skills and teamwork. With two of our team members being first-time hackers, we gained insights into how to push a manageable project within a tight 36-hour timeframe. This experience taught us the importance of effective time management and collaboration. We also learned how to integrate OpenAI into our platform, which was a significant technical achievement. Additionally, we gained experience in designing engaging avatars and user interfaces using Figma, enhancing our ability to create a visually appealing and user-friendly application. Overall, this project not only expanded our technical knowledge but also strengthened our problem-solving and teamwork skills.\nLooking ahead, we aim to enhance Prospera by making the resources provided by Foxy downloadable in a pamphlet format. This will allow users to access important information and resources on their local devices, making it easier for them to refer to the materials whenever needed. Additionally, recognizing that our platform is catered to immigrants, we plan to implement multiple language options to break down language barriers. By offering the app in various languages, we can ensure that more users feel empowered and supported in their financial journeys, ultimately fostering a more inclusive and accessible experience for all.\nShellHacks2024\nDomenica Simbana Mosquera started this project \u2014 6 days ago\nLeave feedback in the comments!\nKristian Correa \u00b7 2 days ago\nI love it!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nHave you ever wanted to trade stocks, but don't have the money to do so? Perhaps, maybe you want to battle it out against fellow traders? Market Melee is the perfect stock sandbox for you!\n\n\n\nOur inspiration for Market Melee stemmed from the entire group taking an Introduction to Computational Investing class. After seeing how a lot of financial jargon, calculations, and visualizations are intimidating to the everyday person, we decided to build Market Melee to make stock market investing more accessible and engaging for everyone. By gamifying the investing experience, we aimed to create a platform where any user can learn, compete, and grow their financial knowledge in a fun environment.\nMarket Melee is a web application that allows users to create and manage simulated stock portfolios using real-time market data. Having hundreds of stocks to choose from, notably from the S&P 500, they are able to allocate their investments and track their portfolio performance over time. Features include interactive graphs, detailed stock metrics, and a place to track your own progress against friends within your league.\nWe built Market Melee using Python and the Flask web framework for our backend, and utilized MongoDB Atlas as our database to store user information, and stock statistics. For our data analysis and visualization, we used libraries such as pandas, NumPy, and MatPlotLib to read stock information and generate performance graphs. The frontend was developed with HTML, CSS, with Bootstrap for responsive design, Javascript to add interactive features, and Jinja to wrap it all nicely with our Flask backend.\nOne of the main challenges we ran into was displaying our data visualization graphs onto our frontend, as we had to decide whether we were to store our images via cloud, storage, or make them temporary images. Furthermore, git version control was another massive hurdle we had to face, as oftentimes the backend and the frontend had conflicting files, which lead to multiple merge conflicts. Authentication and managing users was also a tough task, as we had to make sure users are only able to see their information, while also storing and utilizing other users' data at the same time.\nWe are proud of creating a platform that helps bridge the gap between stock investors and the everyday person. Furthermore, we are very happy about how our main features of individualized stock statistics and visualizations worked seamlessly with our frontend, which allows users to view individual stocks and learn more about them, using pop-up modals to learn more about stock statistics, such as daily returns, volatility, and even the Sharpe Ratio.\nWhile developing Market Melee, we improved our ability to work with a foreign stack for web development, as navigating through this was slow at first. Furthermore, we learned how to effectively leverage document-oriented databases like MongoDB and Flask for routing and session management. Furthermore, we strengthened our ability to delegate tasks in order to have a smooth work flow and proper code management.\nLooking in the future, we plan on incorporate more league functionality such as live-server drafting between users to mimic for users who want to mimic actual drafting of limited stock options. We also hope to implement machine learning and more advanced algorithms to help users leverage technology to their advantage to profit from the stock market. In addition, we would want to implement more educational resources within the platforms to help users grow financial literacy and their own portfolios.\nHackGT 11: Circus of Inventions\nVenn Reddy started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nmonitoring dashboards, uniquely effective to each problem\n\n\n\n\n\n\n\nMonoMonitor was inspired by the need for an efficient tool to monitor multiple developer workflows in one place. Developers often have to juggle between various platforms like Vercel and GitHub to track logs and activity, leading to context switching and inefficiency. MonoMonitor streamlines this process by allowing users to simply prompt the app with a command like \u201cshow me my Vercel and GitHub logs\u201d and instantly get an intuitive UI displaying live logs from both platforms. The aim is to make monitoring as effortless as typing a natural language prompt, saving time and boosting productivity.\nMonoMonitor integrates multiple development platforms such as Vercel and GitHub, fetching and displaying logs in real-time. Users can input prompts to monitor their workflows, and MonoMonitor generates a dashboard with log cards streaming data from specified platforms. This allows developers to view all their important logs in a unified interface, reducing the friction of managing multiple sources of information.\nHad four goals:\n\ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89 Woohoo\nMonoMonitor is built using Next.js for the frontend, with Drizzle ORM managing database interactions and Chakra UI for a sleek, user-friendly interface. We integrated the OpenAI API to process user input and transform it into log-fetching commands. The Vercel and GitHub APIs are used to stream log data, which is displayed in real-time on the app's dynamically generated cards. Additionally, Drizzle Kit was used to handle migrations and schema evolution.\nOne of the main challenges was handling real-time streaming of logs from different platforms without overwhelming the UI. Ensuring the stability of the log streams, especially when combining multiple sources, proved difficult. Integrating APIs from both Vercel and GitHub in a unified way, while keeping performance optimized, was also a complex task. Balancing real-time updates with the need for a smooth user experience required careful optimization.\nWe are proud of successfully creating a unified monitoring experience that simplifies a key pain point for developers. The seamless integration of different platforms into a single view, combined with a natural language interface, was a significant achievement. We also managed to build a highly efficient, real-time system that is responsive and intuitive, helping developers stay focused on their core tasks.\nThroughout this project, we learned a great deal about real-time log streaming and handling multiple API integrations. We also gained deeper insights into optimizing UI components for real-time data rendering and balancing performance with functionality. Additionally, working with Drizzle ORM and Drizzle Kit helped us understand how to build more maintainable and scalable database solutions.\nIn the future, we plan to expand MonoMonitor's capabilities by integrating more platforms such as CircleCI, Netlify, and AWS. We also want to add advanced filtering and alerting features, so users can set up custom notifications when specific events occur in their logs. Expanding the prompt processing abilities to handle more complex user commands and improving performance for even larger scale log streaming are also key goals.\nMHacks 2024\nAudrey Chen posted an update \u2014 5 days ago\ngithub uwu https://github.com/audgeviolin07/Monotor\nLog in or sign up for Devpost to join the conversation.\nAudrey Chen started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nget your right angles\n\n\n\n\n\n\n\n\n\nEver dread having to take pictures because people can never seem to get your angles right? You hand over your phone, spend ages just explaining how you want things to be, treading the line between instructions and being ungrateful, just to end up with 258 terrible pictures in your photo gallery. Not one perfect picture.\nBut don't worry now! We are here to help you always capture your right angles.\n-Andy Vo (Full-Stack Developer)\nAndy is our all-around full-stack developer who seamlessly integrates both front-end and back-end systems. His expertise ensures that every feature of the 90\u00b0 app functions smoothly, providing users with a reliable and dynamic experience.\n-Katniss Min (UI/UX Designer)\nKatniss is our creative UI/UX designer, crafting intuitive and visually striking interfaces. Her attention to user-centred design makes 90\u00b0 easy to navigate, ensuring a seamless and enjoyable experience for users.\n-Leo Zheng (Full-Stack Developer)\nLeo brings technical versatility to the team as a full-stack developer. He handles everything from UI functionality to server-side logic, ensuring that our app delivers an optimal user experience from start to finish.\n-Mythri Muralikannan (AI/ML Dev Ops)\nMythri specialises in ML/AI deployment and automation, providing the intelligence behind 90\u00b0. Her expertise ensures our AI-powered features run efficiently and adapt to user preferences, making every shot perfectly aligned with individual needs.\nWe\u2019ve all experienced the frustration of handing our phone to a friend for a photo, only to end up with hundreds of unusable pictures. Explaining your preferred angles can be awkward, and no matter how detailed the instructions, the result is often disappointing. The process is time-consuming and tedious, leaving you with less-than-perfect memories. 90\u00b0 addresses this issue by providing real-time, AI-driven guidance for photographers, ensuring that every shot captures your best angles without the need for long explanations or repeated attempts.\nUse 90\u00b0 to upload your favourite photos, get guidelines when taking photos, and end up with the perfect photos!\n90\u00b0 is an app that provides innovative photo solutions, allowing people to take the perfect photos based on the perfect angles.\nThe app interface is pretty straightforward and super user-friendly. Let's break it down step by step:\nCreate an account: Start by downloading our app and creating an account with your email.\nUpload new photos: Upload all the photos you took. You can categorise them into our pre-made categories, such as \u201cfull body\u201d, \u201chalf-body\u201d, or you can also make your own category.\nLike your photos: Sort through the uploaded photos and choose your favourites. Our machine learning program will learn your photos to provide you with the perfect guideline.\nTake new photos: Open up the camera and try to take a picture. Guidelines appear on the screen to help as a visual aid to the person with the camera. All they need to do is make sure that you are within the guidelines and then click a bunch of pictures!\nSo how does 90\u00b0 answer these Problems?\nAutomated Angle Detection: 90\u00b0 uses a Machine Learning model to identify your best angles based on your preferences, ensuring every shot is your favourite.\nCustomizable Preferences: You can set up specific angles and framing preferences, so anyone using your phone will capture photos exactly how you want them, without long explanations.\nGuided Photography: With real-time guidance, 90\u00b0 directs the photographer to position the camera at the optimal angle, minimising the risk of bad shots.\nTime-Saving: No more sifting through hundreds of unusable photos - 90\u00b0 guarantees the perfect picture in fewer attempts.\nUser-Friendly Interface: The app is simple to use for both the subject and the photographer, ensuring a stress-free photo-taking experience.\nOur thought process for developing 90\u00b0 was driven by a user-centric approach, as we wanted to address the frustrations people experience when getting the perfect photo.\n1. Empathise: Having experience with endless struggles to communicate our preferred angles and getting unsatisfactory results after multiple attempts really helped us get in the shoes of our app\u2019s users.\n2. Define: The core problem was how difficult it is to consistently capture flattering photos without wasting time and effort. This helped us focus on creating a solution that would make the photo-taking process intuitive and reliable for both users and photographers.\n3. Ideate: Our team brainstormed multiple solutions to simplify the process. We explored using AI and machine learning to detect ideal angles, personalised user preferences, and real-time guidance for photographers.\n4. Prototype: We built a functional prototype of 90\u00b0, integrating customizable preferences and guided photography features. This allowed us to test the major functionalities in a real-world setting.\n5. Test: To make sure that our app runs seamlessly, we will be testing with more users and fine-tune the app based on the feedback we receive. We will gather feedback from multiple potential users, and make sure to address their needs effectively. Through such, we aim to be the app people can rely on to take their stress out of photography.\nThe Backend utilises MongoDB to store data and Mongoose to integrate MongoDB interactions with JavaScript. We used three important endpoints for our application: one for user registration, one for logging in users, and the last for uploading and storing images. To ensure the safety of our users, we used bcrypt to add a salt to the passwords in case our database is accessed by a malicious actor. We also implemented AWS as part of our Backend to store user images, allowing quick access of data using AWS servers and secure storage of user information.\nOur front end comprised two distinct components: Figma and React Native.\nIn Figma, we crafted a prototype to visualise the app's design, focusing on branding and personal toolkit elements. React Native then materialised this prototype into our fully functional app.\nUser\nTerraform\nMongoDB\nMATLAB\nAdditionally the pictures clicked are automatically stored in the User's ohone gallery.\nOur model effectively combines deep learning (for feature extraction via AlexNet) and classical machine learning (SVM) to classify the images in your dataset.\nhttps://drive.google.com/file/d/1AoAbIwdtFBxKC9_TU8Agc15NkmT_UZ3B/view?usp=sharing\nWhy did we use Terraform?\nOur Motive\nOur Uses Our final deployment model is called final_deploy_terraform and the rest are just initial versions that we tested out python and matlab models on. We had a few more EC2 instances but that data was lost and we don't have access to any pictures anymore.\nhttps://drive.google.com/file/d/1I6-bOCwL-JyrnUvwhjeBCEymVhUD_GNf/view?usp=sharing https://drive.google.com/file/d/1Kw85iNO44HHcxagics6f_k16qZXf_qwt/view?usp=sharing\nWhy MATLAB, not Python?\n4 different scripts\nThroughout our hackathon journey, integrating all the components of 90\u00b0 was the biggest challenge for the team. Bringing together the front-end, back-end, AI/ML models, and deployment processes in a coherent matter brought several issues and technical blocks to our team. Ensuring that the AI-driven angle detection interacts smoothly with the user interface, while also managing real-time camera functions, proved to be complex. We found that high volumes of localised code and datasets were not supported by Github, which made the whole process a little difficult as well. Moreover, we also lost some data along with the ec2 server, which caused difficulties as well.\nAdditionally, one of our teammates\u2019s AWS \u2018super secret key\u2019 accidentally got leaked onto a public GitHub repository, which caused an unexpected delay in our hacking journey. Luckily, we were able to fix it as soon as possible and did not have further security breaches. Through this situation, we learned the critical importance of safeguarding sensitive information, such as API keys and credentials, especially when collaborating on public platforms like GitHub.\nDespite these insane situations, we were still able to come through all situations and display our final app - 90\u00b0.\nHere\u2019s the link to our Figma prototype: https://www.figma.com/proto/POstEeFOL4tMNcIxw6VO98/90-degrees?node-id=55-3&starting-point-node-id=55%3A3\nIf you want to run this yourself, clone the repository and run \"npm install\" in the server and app directories. You can run the mobile app with \"npm run start\" and the server with \"npm run dev.\" For the server to work, you will likely need to create your own firebase project and link it with a JSON certificate.\nWe made this app for anyone who wants good photos of themselves. Just download our app, select your favourites, and hand the phone over to anyone for the most perfect photo of your life.\nThrough the development of 90\u00b0, we learned the importance of seamless collaboration between various tech components, such as front-end, back-end, and machine learning models, and how to overcome challenges in real-time deployment. This was a new experience when it came to handling large local files and overall integration. We were so used to relying on Git that this was a new challenge when it was too large to be supported.\nWe also gained valuable experience in securing sensitive data, handling unforeseen issues like AWS key leaks, and understanding the power of Terraform for managing scalable infrastructure. Contacting the AWS staff made us understand the risks and possible threats to personal information that could be created when such data is leaked.\nThe project also deepened our understanding of AI integration for user experience design, focusing on creating user-friendly and innovative solutions. It was our first time working with such large sets of data, and we realized the importance of the computational power of the platform we were using, thus we made the decision to switch from Python to MATLAB, which resulted in a more accurate and efficient model. The training time drastically changed from 35 minutes to barely 2 minutes. The stress on the processors was also drastically reduced by the lightweight matrix storage provided by MATLAB.\nWe are proud of integrating everything successfully onto Terraform for deployment, which shows us a bright future for our next steps. Our AI model also worked successfully on a large group of users (a bunch of our friends!), and the data training process was seamless.\nWe are proud of successfully building and launching the 90\u00b0 app prototype with a successful log-in, user authentication which is backed by MongoDB, along with a fully functioning camera feature which saves pictures into AWS. The smooth integration of our AI-driven angle detection, the user-friendly interface, and overcoming significant challenges like real-time deployment and data loss were remarkable accomplishments.\nDespite setbacks, such as losing server data and encountering security breaches, we persevered and showcased a functional, innovative app that delivers on its promise of providing you with the right angle.\nHackGT 11: Circus of Inventions\nWorked on developing Figma and website, focusing on UI and UX.\nHandled everything from UI functionality to server-side logic, ensuring that our app delivers an optimal user experience from start to finish.\nWorked on developing, training and testing the ML models on Python and MATLAB. Additionally, did DevOps for deployment and integration.\nkatnissmin Min started this project \u2014 6 days ago\nLeave feedback in the comments!\nRoland Saavedra \u00b7 6 days ago\nwow! this is really cool\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nFinancial literacy done smart, together.\n\n\n\n\n\nWe are hunting down the feeling of uncertainty when reading financial documents. Our team understands how difficult it can be to understand complex documents. As college students, the big financial jargon can be daunting. Additionally, many of our teammates are from families whose first-language is not English - for them, too, this task can quickly lead to an overwhelming confusion of words. The situation is exacerbated for underrepresented individuals. Due to the lack of understanding for financial documents individuals are turned away from pursuing their business pursuits or end up making a poor decision. The current market does not have a streamlined way for individuals to be able to better understand their financial matters. We sought to create a platform to centralize the process in improving the community\u2019s financial literacy by providing guidance & document summaries.\nOur web application is revolutionizing the way users take control of their financial futures by offering personalized recommendations and tailored financial advice like never before. We\u2019ve created a one-stop hub where financial literacy becomes not just accessible, but empowering. Whether users are navigating tricky financial terminology or seeking guidance on complex financial projects, our platform provides expert insight with the user\u2019s unique financial history and demographic profile at the core of every recommendation. With our intuitive PDF upload feature, users can upload documents they need help deciphering, and our system doesn\u2019t just analyze\u2014it highlights key sections and offers easy-to-understand, personalized definitions of the most complex terms. But we didn\u2019t stop there. Our AI-powered recommendations go beyond generic advice, delivering insights that are custom-fit to each user\u2019s background and financial circumstances, helping them make smarter, more informed decisions. By centralizing financial education into a seamless, user-friendly experience, we're not just improving financial literacy\u2014we're building the financial confidence that drives lasting change in our communities.\nThis web application was built using a robust backend architecture powered by AWS and Firebase, combined with a modern frontend designed in TypeScript and CSS. Leveraging AWS services like S3 for document storage and Textract for extracting text via machine learning, the system seamlessly processes user-uploaded documents. A Lambda function, triggered via API Gateway, handles uploads, initiates Textract processing, and returns structured JSON outputs, all while maintaining CORS functionality. The backend also integrates IAM roles to secure access across S3, Textract, and Lambda, with Cloudwatch for real-time monitoring and logging. On the user management side, Firebase Auth handles authentication, while Firestore stores user data and documents in a well-structured database. Additionally, OpenAI\u2019s models like GPT-4o provide personalized financial analysis and advice based on demographic data. Node.js and Express.js power server-side operations, handling HTTP requests and routing efficiently. The frontend was developed in TypeScript, styled with CSS, and designed using Figma for a smooth user experience, integrating tools like Axios, PDF Viewer, and Highlight for enhanced interactivity and document display.\nTo preface, none of our teammates have been exposed to the majority of backend tools used in our project (AWS, Lambda, Textract, Firestore, Express.js, Axios, Typescript). Overcoming this major learning curve meant an in-depth conversation with the team on our strengths, weaknesses, & current interests in learning. Our team was completely unaware of how to transfer from Cognito (AWS\u2019 authentication service) to Firebase Authentication (Google\u2019s service). The SDK\u2019s from the two platforms have different APIs, making the authentication, session verification, and user management difficult as significant refactoring was necessary. Another challenge was saving the users session into their user dashboard. It was difficult to return to an exact session & redisplay that screen from the database where the suggestions, chats, as well as PDF annotations were saved. Frontend wise, it was difficult to create smooth transitions onto the page & then create a logical flow of Typescript components. There was a constant need for rerouting as well as a process with trial & error to position the elements/create smooth transitions.\nWe are beyond proud of your ability to preserve through a very daunting task and execute accordingly on it. We spent hours before beginning our project researching our target aduice\u2019s main motivation for participating in the web-app. In that timeframe, we also planned into detail what the flow of our application would look like - even having the opportunity to wireframe with Figma. Due to this process of test-driven development, we were able to reduce the amount of time it took to create certain aspects.\nOur number one accomplishment in the backend was being able to integrate every component together to create the document analyzer. Using our API from the gateway, our document was accessed on S3 with our lambda function, which used Textract to turn the text into a JSON file, which in turn was processed by OpenAI's GPT, storing both the pdf and the JSON within our Firestore database, associated with each user's individual account from the authentication. On the user side, they simply clicked a line and would get a definition and an analogy that accounted for their demographics.\nFrontend's biggest accomplishment was being able to design the elements on Figma & bring that vision to life. Our team self-curated figures to use on the pages, too. It was an amazing opportunity to see how the design of different elements completely changes the user\u2019s experience as well as their perception of which elements to click. When we finally established the design for the page to scan documents, we were overjoyed because of the amount of little details that mattered into the overall functioning of the page. For example, adding pulsating effects to draw the user\u2019s eyes to an important fact.\nThe biggest thing we learned was how to be resourceful together as a team but also individually.\nAs a team we had to truly work with each other's strengths and present ourselves in a vulnerable state to admit our weaknesses. By playing off what had a knack as well as our own interests in learning something new, we were able to incorporate our group member\u2019s interests and passions into this project.\nIndividually, we were able to learn how to look for specific articles/videos/comments online and synthesize that information to become what we have created. Understanding that the answer cannot be found in one location, but rather a harmony of several became a huge growth point in this hackathon.\nIn the future, we would love to tailor our responses more towards underrepresented communities. This would include training models to identify biases in the information we are presenting. Moreover, we\u2019d like to incorporate a section that would present opportunities for those individuals based on the demographic data we collected. To help a wider audience of individuals, it would be neat to include a translating option to convert the document uploaded into another language. As an additional feature, we could also provide our personalized answers in that language, too. For the scanning capabilities, we would like to implement individual words as well as summary paragraphs.\nShellHacks2024\n- Developed Lamba function with Textract\n- Implemented Firebase storage for documents into our app\n- Implemented React-pdf-viewer to display interactable highlights\n- Connected textract output to React-pdf-viewer and GPT prompt\n- Aided with frontend routing, component relations, and HTML/CSS edits\n- Led frontend development with TypeScript\n- Created modular, reusable components\n- Designed responsive UI with advanced state management\n- Implemented optimized rendering logic for user interactions\n- Utilized CSS for cross-browser compatibility\n- Developed wireframes in Figma for interactive prototypes\n- Aligned visual design with user experience best practices\n- Aided in integrating RESTful APIs via Axios for data handling\n- Created S3 Bucket\n- Implemented Roles in IAM\n- Developed primary Lambda function with Textract implementation\n- Created API with Lambda function\n- Initialized Firestore database\n- Created GPT integration in Node.js\n- Purposed GPT output to correspond with highlighted terms\n- Created Chatbot (front-end and back-end)\nArossa Adhikary started this project \u2014 6 days ago\nLeave feedback in the comments!\nCarlos Hernandez \u00b7 5 days ago\nThis is a truly inspirational mission and project!\nJulia Smerling \u00b7 5 days ago\nSuch an incredible and well thought out project!\nshantaxyz \u00b7 5 days ago\nGoood luck we r rooting\nColin Brumbach \u00b7 5 days ago\nthis is such a cool concept!! The UI already looks incredible, and the uploading is better than most websites I use. You got this team UF!!!!\nJude Kuffour \u00b7 about 7 hours ago\nwhy r u so cool\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nCreating an aware and safer world by identifying emergencies which can improve mobility, public spaces, open areas, school buildings and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublic transport is essential for millions of people around the world, but safety concerns remain prevalent. Sudden stops, starts, or turns can cause passengers to lose balance, especially if they are standing or moving around the vehicle. This issue is more significant in crowded buses where holding onto support bars might not be an option.\n\nAccording to recent studies, approximately 47% of bus accidents result in injuries (Kyle F., 2023). Older adults are particularly susceptible to falls on buses due to balance issues, vision impairments, and slower reaction times. This demographic is at a heightened risk, as falls are the leading cause of fatal and nonfatal injuries among older adults, according to the Centers for Disease Control and Prevention.\nCurrent Solutions on the Market\nWhile several fall detection devices exist, like the Medical Guardian MGMini Lite and the MobileHelp Micro, most are designed to be worn as pendants or wristbands (Habas et al., 2024). These devices focus on individual users rather than addressing the environmental and situational challenges in public spaces.\n]\nAnother solution, the Lunderg Chair Alarm System, uses wireless bed and chair sensor pads to alert caregivers before a fall happens. While effective in home settings, it falls short in complex environments like buses, where factors such as sudden stops and variable space require more adaptive solutions.\nWe are built differently\nOur Solution: EmergencyAct\nEmergencyAct is a real-time, video-based fall detection system designed specifically for public transport. Unlike traditional wearable devices, our solution leverages advanced camera systems integrated with machine learning algorithms to detect potential falls or sudden passenger movements, offering broader coverage and higher accuracy.\n\nDuring the hackathon we managed to somehow tackle the issues by keeping our implementation internal, at least while using FIU\u2019s internet connection. As for the integration of various services we managed to have a running MVP for our demo.\nCollaboration:\nBuilding the project required a diverse set of skills such as computer vision, machine learning, data visualization, and real-time integrations. We learned how to better coordinate among team members with varied expertise, focusing everyone's strengths and knowledge for a better logisitic.\nComputer Vision:\nWe gained hands-on experience with computer vision models like YOLOv8mPose. This project pushed us to go beyond object detection, which are critical for fall detection in complex environments.\nGenerative AI and NLP Integration:\nIntegrating natural language processing models into computer vision workflows was a new territory for us. Combining vision-based fall detection with text-based cause analysis gave us a new understanding of hybrid AI systems and how generative AI can enhance context understanding.\nHuman-Centric Design:\nPublic safety in transport is a sensitive issue. Through this project, we learned to approach safety-critical applications with a human-centered mindset, ensuring our solution is both effective and respectful of user privacy.\nAdapting in Real-Time:\nThe dynamic nature of a hackathon forced us to rapidly adapt to unforeseen issues, such as limited internet access and integrating various components within tight deadlines. We became more adept at troubleshooting, quick decision-making, and agile development, ensuring that we still met our key objectives.\nThese learnings not only enhanced our technical capabilities but also deepened our understanding of real-world applications of AI in public safety scenarios.\n\nNext, we plan to enhance EmergencyAct by integrating it with real-time emergency response systems and piloting it on public buses to validate its effectiveness in real-world conditions. We will focus on improving privacy-preserving techniques, such as Federated Learning, and expanding multi-language support to make the solution accessible to diverse communities. Additionally, we aim to build a centralized dashboard for transport authorities, enabling them to monitor safety trends and generate reports.\nOur streamlit web page is limited due to internet connection restrictions inside the university, so not all features are available.\nShellHacks2024\nH\u00e9ctor Eduardo Tovar Mendoza started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\n\"Empowering Latino Businesses, One Investment at a Time.\"\n\n\n\nThe idea behind Mercado was sparked by the desire to address a gap within Latino communities\u2014particularly Latino-owned small businesses. We often see these businesses struggling to access the same investment opportunities as others. As a result, many never get the chance to scale up. I wanted to create a platform that empowers not only business owners but also everyday people from the community who want to invest in and support local Latino businesses. Mercado represents not just financial markets but also local businesses\u2014our \u201csupermarkets\u201d of culture and innovation.\nDuring Latine Heritage Month, this felt like the perfect opportunity to combine technology with community values to close the digital divide and support local entrepreneurs.\nMercado is a micro-investment platform that allows users to browse and invest in Latino-owned small businesses. It connects local businesses with investors who want to support their growth. Users can filter businesses by category, view their fundraising goals, and contribute investments in small amounts. The platform also has a community feature to foster networking and collaboration between business owners and investors.\nWe built Mercado using:\nThroughout this project, we learned a great deal about building community-driven applications:\nShellHacks2024\nSantiago Moyano started this project \u2014 6 days ago\nLeave feedback in the comments!\nHeldanna Solomon \u00b7 5 days ago\nI knew you were going to win this prize! (I sat next to you guys at the judging table) Congrats!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nFairShare: Get Your Piece of the Financial Pie with AI-Powered Expense Splitting!\n\nApp home page\n\n\n\n\n\n\n\n\n\nGrowing up in immigrant households, many of us witnessed firsthand the financial challenges our families faced as they adapted to life in the U.S. Navigating an unfamiliar financial system came with a steep learning curve, and those early experiences inspired us to make financial literacy more accessible. We wanted to create something that would help families and children alike\u2014people who may be facing similar growing pains\u2014understand and manage their finances with greater ease.\nWorking on FairShare has been an incredible learning experience for our team. We deepened our knowledge of React Native and honed our skills in JavaScript and CSS. Throughout the process, we learned how to build a mobile app from the ground up, how to effectively integrate APIs, and how to craft user-friendly tools that address real-world financial problems. One of the most interesting aspects was diving into expense splitting, something that resonates with anyone who has ever had to share a bill.\nWe developed FairShare using Visual Studio Code (VSCode) with React Native as our primary framework. The core functionality was implemented in JavaScript, and the design was styled with CSS. A major feature of the project was integrating Azure AI, which allows users to receive personalized financial insights based on their transaction history. The AI fetches and analyzes transaction data, giving users recommendations to better manage their financial well-being.\nThe journey came with its share of obstacles:\nGet Your Piece of the Financial Pie with AI-Powered Expense Splitting!\nShellHacks2024\nI worked largely on the frontend renderings of the app's different pages (i.e. expenses, account balance, etc). It was exciting and jarring all the same, especially seeing as how it was my first time doing frontend work and coding in CSS/Javascript using React Native\nI worked on the design aspects of this project, determining its visuals and aesthetic decisions like color scheme and layout. The screencaps pictured were made in Figma by me. I also created some of its pages in HTML and CSS for my teammates to then implement in React Native\nI worked on part of the backend for the emailing system, and refining the React backend, connecting the various components together. I also worked on individual components, including the bill share system, and all the components that come together to make it work.\nI worked on managing the backend emailing system and the backend of the app for managing the users expenses.\nIvie Imhonde started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nReducing Mohs Surgery errors by leveraging intelligent telemedicine, Noma revolutionizes care with real-time 3D visualization of patient faces and automated medical transcriptions of procedures.\n\n\n\n\n\n\n\nDuring our research into healthcare disparities, we uncovered a critical issue: a significant gap in the availability of specialized medical professionals, especially in rural regions of the U.S. This shortage is particularly concerning when it comes to dermatologists performing Mohs Micrographic Surgery (MMS), a precise procedure used to treat skin cancer like Melanoma. Due to this lack of access, rural patients not only experience treatment delays but also face a higher risk of procedural and surgical errors during their treatment.\nOur inspiration for Noma emerged from the potential to reduce these errors through telemedicine. By providing surgeons with real-time 3D visualization and automated medical transcriptions, Noma enhances surgical accuracy and ensures that patients, regardless of their location, receive expert, error-free care.\nNoma addresses these challenges by enhancing communication between surgeons during Moh's surgery.\nReal-time 3D Facial Visualization:\nAutomated Medical Transcriptions:\nTogether, these features enable remote collaboration between surgeons in rural areas and specialists, ensuring high-quality care.\nTo build Noma, we used several advanced technologies:\n3D Visualization:\nAutomated Transcriptions:\nBy combining these tools, we created a system that captures real-time events and generates highly accurate medical documentation.\nHackGT 11: Circus of Inventions\nhttps://youtu.be/8hLX6RIFNMs?si=XIyLv0OOE0TcJKus\nSiddhant Agarwal started this project \u2014 6 days ago\nLeave feedback in the comments!\nKrishnav Singhal \u00b7 5 days ago\ncongratulations!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nLotion is the Notion Killer. Lotion provides slippery smooth notes supercharged by AI and AR. Our immersive platform combines Augmented Reality and AI to make assisted learning available to all!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe came up with this idea when one of our teammates was studying for statistics before the beginning of the Hackathon. He realized that he was spending ages and ages navigating through the uncountable amounts of hopelessly long notes he had taken for his first exam.\nThus, we had the idea to create an AI powered note taking platform that classified, stored, and retrieved notes whenever you needed. Not only does it take away all the headache of searching through your notes by providing a note querying system based on your questions, it has an immersive AR based system that scans your homework to return the most relevant notes!\n\u2728 Note Classification: Automatically classifies and stores your handwritten or typed notes based on content and the classes you are taking\n\ud83d\ude80 AR Note Annotations: Overlays relevant notes based on scanned information from your camera\n\ud83d\udd27 Note Querying: Returns relevant notes based on text input questions or statements\n\ud83d\udcc8 Interactive Interface: Ease of access to all taken notes\nUsing React Native, we created a fully functioning note taking app. In the Flask backend, we used Google's Cloud Vision API to process handwritten notes into processable text to be read using the ChatGPT API. A combination of Firebase for authentication and MongoDB for document storing/querying was used. We tied it all together into an AR feature using ExpoGo's Camera tools.\nOne of our major bottlenecks was connecting our backend and frontend. This was a result of poor communication, perhaps because we didn't sleep. We worked around this by creating a quick api documentation and having a meeting to discuss what endpoints the frontend needed.\nWe also had issues using FireStore because of the collection - document system, which we didn't need. We switched to MongoDB for its simplicity.\nWe are proud of interlinking so many moving parts into one working product. It was satisfying seeing every endpoint, react component, and api call come together as we finished our AR feature.\nWe learned how to use Google's API\nIf we get positive impact, we would push out a more mature version; we hope students could actually use this product.\nMade with \u2764\ufe0f from Team Lotion\nShellHacks2024\nI worked on the authentication and AR camera/Computer vision. I used Expo/React Native + Firebase auth + Motion sensors.\nMy favorite part was doing all the fun math for calculating motion and stability! The ML/Vision part was also really cool too.\nI developed a dynamic and scalable database using MongoDB to efficiently store and manage JSON files enabling the application to handle real-time data processing for improved performance and scalability.\nI developed the flask backend, integrating the Google Vision AI and ChatGPT AI.\nHugo Liu started this project \u2014 6 days ago\nLeave feedback in the comments!\nAnton Salvador \u00b7 6 days ago\nI loved using Lotion\nAdrian Moreno \u00b7 6 days ago\nThis was gamechanging both for my mind and body\nTimothy Kareng \u00b7 3 days ago\nAmazing project. Use Lotion!!!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAn Adobe Express add-on that can transcribe and translate your video's audio into a different language.\nAs a video editor myself, I sometimes find myself wishing that I could translate my videos that are spoken in English into a different language, such as Spanish or Mandarin, in order to reach a further audience and even to practice my own familiarity with the language. Additionally, most YouTube videos have automatic captioning generated, but this is easier for people to caption their videos themselves and even gives them the option to translate their video transcript into a different language if they'd like.\nThis add-on takes a video put into Adobe Express and first transcribes the video's audio into text. The user then has the option to translate the given text into another language, provided that that language is an option.\nThe elements of the add-on's UI were made and developed using Adobe's modified version of React code (React Spectrum and React Aria) and the translation and transcription services were modified from Amazon's initial AWS translation and transcription.\nGetting stuck when coding elements of the add-on's UI, dealing with new and unseen before error messages, and learning new technology and software from Amazon's AWS.\nPersevering through our first hackathon, learning and debugging more aspects of React and TypeScript, and being able to connect all of the different parts of our project together.\nHow to use Amazon's AWS transcription and translation services, coding in React and TypeScript, team building, and planning out and finishing a project in a short period of time.\nReal-time captioning that immediately follows spoken audio in the video instead of producing solely a transcription and translation of the video's spoken audio and possible extensions to captioning and translating solely audio without a video requirement.\nsunhacks\nI worked on the AWS Lambda and our other AWS services (Amazon Transcribe and Amazon Translate) in order to add the functionality needed to complete the goal of our project.\ncrystal lee started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nTinder but for Investing: Making inclusive, value-driven investing accessible to all!\nThe inspiration for our project comes from recognizing how difficult it can be to make informed investment choices, especially for those from underrepresented communities. With so many options and differing financial philosophies, navigating the investment landscape can feel overwhelming. We wanted to create a platform that empowers users to understand these complexities and find a path that aligns with their values. By offering diverse perspectives on finance\u2014sustainability, capitalism, and feminism\u2014we aim to make financial literacy more accessible and engaging, ultimately helping people feel more confident in their investment decisions.\nInspired by the belief that financial opportunities should be accessible and inclusive for all, we\u2019ve created a platform where advisors\u2014each representing capitalism, female empowerment, and sustainability\u2014engage in a real-time debate about the pros and cons of stocks. As you navigate through this interactive experience, you can match or reject stocks based on your own preferences, ultimately building a tailored profile that reflects your investment style. With a personalized dashboard tracking the performance of your matched stocks, you\u2019re not just investing\u2014you\u2019re aligning your financial future with your values. Our vision is to empower users from all walks of life to take control of their financial journey, making inclusivity and sustainability cornerstones of modern investing.\nFor the front end, we used React with Tailwind CSS for a responsive and customizable UI. To create the 3D bird models, we used Spline making the app more engaging. We use PropelAuth for user authentication. We also incorporated streamlit to create a stock dashboard that displays real-time data. We used an Elegoo circuit for taking in user data via a toggle and buttons (toggle to select bird, buttons to accept/decline the stock). For the backend, we used Python with ElevenLabs for the text-to-speech multithreaded with openAI API calls. The server that connects the front end to the backend was created via Flask.\nOne of the key challenges we encountered was managing merge conflicts due to complex file organization, especially as we integrated various features across multiple branches. Working with 3D modelling using Spline and handling animation presented difficulties in achieving smooth transitions between advisor states. On the hardware side, incorporating a joystick added complexity in terms of input handling and compatibility with the software. We also struggled with implementing the real-time 'debate' feature between advisors due to delays from API calls - we were able to solve this by implementing multithreading to handle concurrent interactions.\nCollaboration: Our team worked seamlessly together, leveraging each member\u2019s strengths to enhance the project. Feature Integration: Successfully integrated diverse features, creating a cohesive and functional application. Interactive Product: Developed an engaging platform that provides users with real-time feedback and insights. New Technologies: Explored and implemented several new technologies, expanding our technical skillset. Hardware Component: Incorporated a hardware component, adding a unique and hands-on element to the experience. 3D Animation: Implemented captivating 3D animations, enhancing the app\u2019s visual appeal and user engagement.\nWe learned A LOT about managing complex integrations\u2026\nWe also discovered free temporary emails are a thing instead of burning through all our known people's email accounts for free tier sign-ups :-))\nIn the future, Flock Stocks aims to offer even more diverse perspectives by introducing additional flock advisors. The platform will feature fully immersive VR environments, allowing users to explore investment opportunities beyond stocks, including commodities, real estate, and more. The ML models will be further refined by training them on niche datasets and providing more personalized recommendations. Additionally, users will be able to set goals and enjoy a gamified experience, making investing both educational and engaging. The platform will also enable users to interject during conversations, leading to more dynamic interactions with the flock of birds. Moreover, users will have the option to select different creatures, allowing them to chat with famous public figures in a virtual setting.\nTechNova 2024\nAshnoor Randhawa started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nRateMySchedule delivers a report card on the quality of your schedule. RateMySchedule will calm your nerves, giving the scheduling insights you need to set yourself up for a great semester!\n\n\n\n\n\n\n\n\n\nEach semester, every college student has to endure the dreadful process of scheduling their next one. For us, as Pitt students, this process can be uniquely challenging. It\u2019s easy to just choose some classes, generate a schedule and call it a day, but to ensure the schedule is optimized involves far more work--switching between maps to determine whether a ten minute break is long enough to walk from Sennott to Infosci, scouring RateMyProfessor to choose the best instructors, and keeping an eye on your credit total to maintain a manageable workload.\nOur team\u2019s goal was to optimize this process, and create a Gemini AI-powered model to deliver insights and advice to Pitt students like us who feel overwhelmed by scheduling their classes. RateMySchedule uses a combination of info pulled from your Pitt Schedule and supplemental user input to calculate ratings in three different areas of your schedule: class break time and feasibility, professor quality and credit workload.\nWe built a full-stack web app called RateMySchedule using a combination of Flask and Python on the backend, and React, Javascript, HTML/CSS and Bootstrap on the frontend. RateMySchedule works by converting your .ics file downloaded directly from Pitt Peoplesoft to a .csv file, which is then parsed to identify your classes and the breaks between them, including length, origin building and destination. Then, you input the names of your professor for each class and the number of credits it\u2019s worth. RateMySchedule immediately gets to work generating your schedule report card.\nFor class breaks, we first collected a list of all Pitt Academic buildings and their coordinates, then imputed these into the HERE Routing API to calculate the distance of a walking route between any given building coordinates. This distance was formulated into walking time, then subtracted from the length of the break. We scored each break on the feasibility to make it between the two classes within the time allotted by the schedule. For professor quality, we used an API from the popular site RateMyProfessor, and calculated an overall rating for professors by combining student feedback on metrics like professor rating, difficulty, and percentage of students who 'would take again'. Credit amounts were rated by totaling them and comparing typical credit loads to determine relative difficulty and manageability.\nIn generating the schedule report card, we implemented the Gemini API to read the data collected and rankings calculated for a given student, and generate personable, intelligent text to provide encouraging and helpful advice to students, as well as explain the reasoning behind their scores. Chat GPT was used to help us understand and handle errors, and assist with CSS styling.\nThere were several challenging aspects of creating this app--we had to work around not having access to the Google Maps API by collecting Pitt coordinate data manually and calculating walking routes through the HERE Routing API instead. We also had to carefully tune our prompts for Gemini API, to ensure it was adding the most accurate and informative text possible to our report cards. This is an area of our project that could be expanded upon boundlessly--with the data we collected for the Gemini API, it could be capable of providing even more specific and helpful advice to users of RateMySchedule. Finally, our team faced the challenge of learning several new tools--for everyone, this was our first full-stack web app project, and our first time working with APIs, AI and advanced front-end development.\nWe are all incredibly proud of what we have accomplished, and feel more prepared as computer scientists to take on full-stack challenges, and build real, useable applications that we are excited to share with the world. We all learned so much about using Github collaboratively, Python, Flask, React, and every other tool that we implemented, no matter how challenging. The greatest learning experience of all was perhaps learning to work as a team to make our idea come to life, which despite the unique challenges it came with, brought us closer together and improved our collaborative coding skills tenfold.\nWe would like to further develop the AI capabilities of RateMySchedule to deliver even more personalized and detailed reports, possibly taking into consideration a student\u2019s major, class difficulty, and ability to return home during longer class breaks. Implementing a Gemini API chatbot to ask follow-up questions on a schedule is also a goal. We would also like to access more advanced routing capability to make even more accurately timed walking routes for our users. Finally, we would like to adapt RateMySchedule, to be available for students at any college, not just Pitt students.\nSteelHacks XI\nJames Toscano started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAn AI application designed to get you the truth about health facts.\n\n\n\nOur project was inspired by doom-scrolling. YouTube shorts, Tiktok, and Instagram reels have boomed in popularity in recent years. So has health-related content involving these short-form content. With so much misinformation, we wanted to find a way to combat this - MediTruth.\nMediTruth takes these short-form videos and extracts information being presented as facts. We then flag these facts as true or false using medical journals and provide reasoning. This provides a way to educate yourself and not succumb to misinformation.\nWe utilized LangChain and Google's Gemini LLM, and PubMed's API to get article data. Next the information was then split and stored in MongoDB where we can look up answers to combat or support the facts presented in the video using similarity search. The frontend was then built simply using HTML/CSS.\nThis was probably the smoothest hackathon we've competed in. While it was mostly smooth sailing, there was some hiccups. Getting the data from PubMed was extremely difficult. A lot of the documents are not readily free and so getting the documents was difficult. This was made even harder when PubMed rate limits their API and the articles are much more technical than the average YT/TikTok video.\nWe are proud of making such an overall polished product. It performs decently well and provides good information to the user.\nWe learned how to create a full-stack application and incorporate AI. It was our first experience using a LLM and there were many struggles prompting it to get the right output.\nWe plan to implement real-time fact checking support so the user does not have to wait until the entire video is analyzed (facts would populate as the video played). Additionally, we would implement the software also as a Chrome extension, which reduces the clunkiness of having to copy and paste a link - the extension could recognize videos and analyze in the background whenever a page is loaded. We also plan to add support for Instagram, TikTok, and other social media platforms.\nSteelHacks XI\nJeffrey Zheng started this project \u2014 6 days ago\nLeave feedback in the comments!\nParth Patel \u00b7 5 days ago\nExample YouTube Videos:\nhttps://www.youtube.com/watch?v=ArFQdvF8vDE\nhttps://www.youtube.com/shorts/oynU6CVDNsY\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nDemocratizing pro-level Poker play in real-time with computer vision\n\n\n\n\n\n\n\n\n\n\n\nhttps://pitch.com/v/poker-face-zrnyfu\nOur Breadboard: https://breadboard-ai.web.app/?tab1=https://breadboard-community.wl.r.appspot.com/boards/@AdorableElephant/poker-face-copy.bgl.json\nPlaying poker is easy. Playing poker well is hard.\nAt PokerFace, we want to make playing poker well easy for everyone by democratizing optimal decision making strategies used by top players across the world and bringing them to your live poker games. Our platform views and identifies your cards, chip stacks, pot sizes, table and bet sizes, using Computer Vision to provide you GTO optimal advice and strategic explanations to make you the best poker player possible regardless of your background. We use computer vision, statistics, probability, and Generative AI to precisely and accurately provide real time insights and explanations on your real life Poker games, allowing you to maintain composure in critical situations. Computed mathematical analysis to compute pot odds, bet sizes, risk strategies, equities, expected values, allow you to build strategic intuition for the beautiful game of poker.\nUsing a combination of AI-driven insights and mathematical analysis, PokerFace delivers real-time data on pot odds, bet sizing, equities, and risk strategies. Whether you're new to the game or an experienced player, PokerFace enables you to maintain composure and execute optimal moves during critical moments by providing detailed strategic guidance based on calculated odds, EV (Expected Value), and other crucial poker metrics.\nOur live-updating dashboard is powered by real-time computer vision that accurately captures and processes in-game information like bet sizes, stack sizes, your cards, community cards, and pot sizes. Based on this data, the system generates real-time recommendations, such as optimal bet sizes, calculated pot odds, risk-adjusted strategies, and expected values, guiding you toward the most profitable moves. Additionally, we don\u2019t just give you the numbers\u2014we also provide strategic explanations for each decision so you can learn from every hand, improving your overall poker skills.\nPokerFace doesn't just optimize your play in real-time; it also stores and analyzes past gameplay data to give you a comprehensive view of your habits and tendencies over time. By understanding your biases, patterns, and heuristics, our system offers personalized insights that help you become a better poker player. Whether you're prone to risk-averse behavior or overly aggressive bluffs, PokerFace\u2019s AI provides tailored advice that addresses your unique playstyle, making you more adaptable in future games.\nOur AI agent, dubbed \u201cLady Gaga,\u201d leverages prompt engineering, recursive contextual models, and advanced machine learning techniques to offer expert-level insights into poker strategies. By processing data gathered from gameplay and image streams, the AI agent can analyze your current situation and suggest optimal moves while teaching poker at a high level, regardless of your current skill. Integrating Google Breadboard and computer vision from a live video feed allows the AI to continuously update itself with the most accurate and relevant data.\nPokerFace was developed as a mobile application using React Native (Expo) for the frontend and FastAPI for backend operations and statistical analysis. We used Roboflow to build our computer vision model, which processes poker chips, cards, and other gameplay elements. For the AI-driven insights, we integrated Google AI Breadboard, enabling our LLM (Language Learning Model) assistant to provide expert feedback, gameplay highlights, and strategic recommendations.\nFrom a mathematical standpoint, we delved into poker theory, combinatorics, and game theory to design accurate statistical models. Monte Carlo simulations were employed to calculate Expected Values (EV), pot odds, equities, and other vital poker metrics based on real-world gameplay data. We also factored in player behavior tendencies\u2014such as risk-aversion or risk-seeking\u2014to ensure our AI\u2019s decision-making mirrors real-life strategic dilemmas.\nWe encountered multiple technical challenges throughout development. Establishing a continuous WebSocket connection for the video stream was complex, particularly given the transition from React.js to React Native for mobile development. Additionally, learning new technologies like Google Breadboard and Roboflow was initially difficult due to unfamiliarity, but ultimately they became crucial to improving the overall quality of PokerFace.\nAnother major challenge involved structuring the input pipeline for Google Breadboard. Initially, the context mapping was handled incorrectly, causing the model to infer the wrong information. By reconfiguring the system to handle different input modes (e.g., image streams and text data), we eventually resolved this issue. Moreover, we struggled with SSE (Server-Sent Events) versus JSON formats, which required a more specialized approach to handle real-time data streams.\nWe successfully integrated a custom AI agent using Google Breadboard that could process player history, contextual data, and game information to deliver expert poker advice. This recursive feedback system allows the agent to learn from each player\u2019s behavior and improve decision-making recommendations over time.\nFurthermore, we developed a full-stack mobile application with a dynamic user interface, a robust backend powered by FastAPI, and a computer vision model that recognizes poker chips and cards in real-time. Our use of advanced mathematical models and algorithms to guide optimal decision-making is another accomplishment we're particularly proud of.\nThroughout this project, we gained extensive experience in integrating AI agents with custom machine learning models. We also learned how to use FastAPI to create a real-time interface for our computer vision model. Additionally, working with Google Breadboard gave us hands-on experience in integrating multi-modal inputs and building a custom LLM tailored to poker strategy.\nLooking ahead, we plan to enhance PokerFace by integrating wearable cameras (such as those embedded in glasses) to improve the user experience and provide a more seamless way to capture gameplay data. We\u2019re also excited to explore sentiment analysis as a way to better understand player emotions, helping users maintain an effective \"Poker Face.\"\nIn the future, we intend to build a replay engine that allows players to review their previous games and learn from their mistakes. This would allow for deeper insights into gameplay performance, creating a powerful tool for learning and growth.\nMHacks 2024\nOm Joshi started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nFocusFlow is an engaging web platform designed to transform online reading into an interactive experience, specifically tailored to support individuals with trouble concentrating or users with ADHD.\n\n\n\n\n\n\n\n\n\nEver struggle getting through online assigned readings? Expected to read hundreds of online pages for research, clubs, or personal study? FocusFlow is the solution to all of these problems! Largely inspired by our teams personal connection to individuals with ADHD and peers with trouble concentrating especially when reading large scale online documents, FocusFlow uses a Machine Learning and Neural Network handled by an Intel AI PC backend, Artificial Intelligence, and a heavily developed Interactive UI to bring efficient online reading to the public.\nADHD and other neurological and mental health disorders have only become more prevalent in recent years. Studies from the National Institute of Mental Health have shown that in the year 2022 7.1 million children and adolescents were diagnosed with ADHD, not to mention other similar disorders such as ADD. Our tool is designed to reinforce learning in those affected, and create efficiency for those that struggle with holding concentration while reading.\nFocusFlow is an dynamic webapp which prompts the user to enter a PDF or any text based document. After this data is entered, our code runs an AI API based algorithm which extracts the data from the source file and displays it in a dynamic UI format on the \"parsed\" page of the website. From there, the user is prompted to calibrate their webcam to their own personal iris movement using a webcam eye tracking library using ML. This library allows us to track where a user is focused on the screen after calibration, and continually trains on machine learning data based off your usage for high accuracy and adaptability despite the hardware constraints of using only a webcam. After calibration, the user is shown their parsed text document in our dynamic UI. As the user reads, text dynamically becomes highlighted through our eye tracking, which allows users to both keep their pace, and stay focused. If our AI were to detect someone's vision becoming unfocused (go on their phone, or some other device off screen), the software will softly change color to prompt the user back to the task. After the PDF is completed, data including words read, and total time spent reading are processed by our backend to provide constructive information on how a user can continually improve their focus. In addition, we have implemented an AI Chatbot using the ChatGPT LLM API inputted with the text, which allows users to gain insightful, personalized feedback on how they are doing. This LLM chatbot, combined with our AI parsing, attention quantifying neural network, and eyetracker ML algorithm, are all ways we have used cutting-edge technologies to address this problem.\nFocusFlow is built with a heavy emphasis on a Machine Learning framework with interactive UI at the forefront of our development. We used a React.JS frontend through Vite, combined with a Node.JS backend for our LLM chatbot API management. We also used an LLM API for intelligent parsing of a wide array of pdfs (papers with latex, diagrams, etc). We then used JavaScript to dynamically highlight text in our UI based off vision tracking from the WebGazer.js machine learning library. Finally, we trained a Long-Short Term Memory (LSTM) Recurrent Neural Network (RNN) regression model using Intel Cloud capabilities to determine if users are focused or not. We generated our own datasets, and trained and tested completely on our own from recorded videos. Finally, to add some fun to our site, we integrated games that can be unlocked after a certain amount of time studying, providing a reward for good focus.\nNext, we hope to see how our eyetracking improves by using an actual eye tracking webcam. We reached the limits of our laptop webcam, and are really excited to see the accuracy improvements that better hardware can make! This is a major reason why we are applying under the accessibility track - the improved webcams would take our webapp to the next level.\nMHacks 2024\nHosted website link coming soon!\nJack Morby started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nSmart Home Meets Spectacles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspired by the futuristic interfaces seen in films like Iron Man, the convenience of smart home hubs, and the intuitive interactions of natural user interfaces, our SnapAR Spectacles project aims to revolutionize home control. By leveraging augmented reality, users will be able to seamlessly interact with their surroundings, using gestures and visual cues to control lights, thermostats, and other smart home devices with ease. Just as Marvel heroes rely on their tech-savvy abilities, users will feel empowered to manage their homes with a touch of superhero flair.\nStep 1: AR Interaction: Users interact with a virtual marker (e.g., light switch) through the AR interface on their SnapAR Spectacles. The marker is detected using Lens Studio\u2019s image-tracking capabilities.\nStep 2: Data Flow: Once the user interacts with the marker, a JavaScript event is triggered, sending data (e.g., toggle light) to Firebase.\nStep 3: Firebase Update: Firebase processes the input and sends a command to the hardware (via Python) to physically toggle the smart home device (like a light switch) connected to a Raspberry Pi.\nStep 4: User Feedback: The result (light on/off) is instantly displayed back to the user through their AR glasses, completing the feedback loop.\nHackGT 11: Circus of Inventions\nDanniecia Gray started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nMed-Memory turns overwhelming medical data into actionable insights with easy-to-read visuals and AI-powered assistance. Helping doctors make smarter decisions that save lives.\n\n\n\n\n\n\n\n\n\nIntroducing Med-Memory: your fast-track solution for patient data. Instead of digging through endless reports, Med-Memory delivers key insights in real time, transforming complex lab results and treatment updates into clear, actionable visuals. With just a few clicks, you get instant, AI-driven answers to your questions, allowing you to focus on what matters most\u2014providing excellent patient care, not paperwork. Save time, make smarter decisions, and ensure better outcomes with Med-Memory, the smarter way to manage patient information.\nMed-Memory transforms overwhelming medical reports into easy-to-read, real-time dashboards. Med-Memory leverages AI to analyze individual patient data, providing doctors with instant, AI-powered insights. The system is trained on each patient's data, enabling it to answer any questions related to the patient's history, treatment, and conditions. This ensures that doctors receive precise, personalized responses that support informed decision-making in real-time.. The tool simplifies decision-making, enabling healthcare professionals to focus on critical care, especially in time-sensitive situations.\nWe built Med-Memory with a Node.js (Express.js) backend and a Firebase database for data storage. On the front end, we used Next.js with React for seamless user interaction. The Qwen 2.5 LLM model was fine-tuned and deployed via Ollama and tested via Streamlit for interactive data analysis.\nTraining the Qwen 2.5 LLM to understand specific patient histories and deploying it locally with Ollama posed significant challenges, especially in optimizing the model for fast, accurate responses. also maintaining more than 4 servers on the local host and maintaining communication between these servers.\nSuccessfully integrating the AI model into the real-time dashboard, allowing doctors to ask questions and receive instant, actionable insights. We\u2019re also proud of optimizing the tool for efficiency in handling complex medical data, making it usable in fast-paced medical environments.\nWe gained experience working with Next.js, Firebase, and LLMs. We also learned the nuances of fine-tuning AI models to handle specific datasets like medical records, which is critical for ensuring accuracy and relevance in healthcare applications.\nWe plan to deploy the fine-tuned model at scale on the cloud, making it accessible to doctors and patients alike. We\u2019re also working on integrating an audio analysis ML model to extract key insights from doctor-patient conversations, further enhancing the tool\u2019s utility in real-time clinical settings.\nShellHacks2024\nTanmay Pajgade started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nAn accessible white-boarding app to voice your ideas into existence.\nAs software engineers, our team has become extremely dependent on white-boarding technologies to help us convey ideas to each other. Without these tools, we realized that collaboration would be much harder, and realized that not everyone is able to benefit from the beauty of collaborative diagramming. Whether it be a physical disability or a disdain for technology, Voiceboard is meant to be accessible for everyone.\nVoiceboard listens to incoming speech and sends the requested drawing in plain text to Google Gemini. With some clever optimizations and prompt engineering, we convert the text into Mermaid, a syntax for creating diagrams.\nWe utilized built-in browser Speech Recognition APIs to help detect speech client side. Then, we sent filtered transcript over to our server once the wake phrase was detected, and generated Mermaid markup to demonstrate the graphs using Google Gemini. We also made sure to send the current state of the whiteboard to allow for modifications, and added other usability commands to the drawing screen. Lastly, we filtered the Mermaid code block, made necessary refinements, and mapped it into an Excalidraw embed.\nThis was our first time working with Speech Recognition and Diagramming technologies, so it was a bit of a challenge to find the right technologies. Initially, we attempted to create an audio stream encoding to a Google Cloud Speech Recognition service, but the technical complexity of constant audio upstream was a headache in itself. As for Mermaid, none of us had any experience working with it. We struggled to find a good Mermaid render engine for React, and had to settle with Excalidraw, which provided lots of good ease of use options but only supported a subset of Mermaid elements.\nWe're proud of how seamless the voice recognition is. We spent a lot of time for making the recognition flow smooth rather than always listening and jumbling up the board. We are also happy that we could provide Gemini the context of the current board and provide a conditional flow that lets users modify boards after they are created as well.\nWe learned a lot about Mermaid and the complexity behind syntax parsing, as well as the complexity of Speech to Text technology. We hope to use these new skills in future projects!\nWe want to look more into Mermaid rendering to make the actual processing of the ideas even better. If time allows it, two avenues we would like to explore are our own rendering engine for the Mermaid code-block or perhaps even a custom trained model on Mermaid.\nShellHacks2024\nDylan Vidal started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nFrom browsing clothes, testing your retro trivia and picking a playlist, Glitz and Glam will make sure you are prepared for a night of disco fun!\n\n\n\n\n\n\n\nOur inspiration was:\nOur website:\nAre pages were built using the following:\nSome challenges that we ran into were:\nAccomplishments that we are proud of:\nGirlHacks 2024\nI worked on the front end part of the Trivia Page. I am proud of the way it turned out.\nI worked on the dress-up game as well as helped with UI on other parts\nNahallah Champagne started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nWell, here comes the MSign, it is a light weighted sign language interative learning site. Whenever you want to learn some sign language, click!\nthe site\nThe inspiration behind Msign comes from the growing need for inclusive technology that helps bridge the communication gap for individuals who are deaf or hard of hearing. With more than 70 million people worldwide using sign language, there is a pressing need for tools that can help others learn and translate sign language into text seamlessly. We wanted to create an intuitive and accessible platform to help both learners and non-signers communicate more effectively using modern technologies like machine learning and computer vision.\nMsign is a web-based sign language learning and translation platform that allows users to:\nThe app utilizes machine learning models to detect hand gestures from the webcam and translate them into readable text, bridging the gap between sign language users and non-signers. How we built it\nSome of the major challenges we faced while developing Msign were:\nThroughout the development of Msign, we learned:\nWe have big plans for Msign in the future:\nMHacks 2024\nI am the full stack developer in this project from front end using css, and to the back end logic using flask and javascript\nJuntao Wu started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nSearch for the song on the tip of your tongue\nAs musicians and music-lovers, we wanted to create a way to bring more people into interacting with music. Originally, we thought of the therapeutic benefits of music to elderly people with dementia. This led to the issue of trying to remember a song without knowing its name, which is how we came up with the idea of SongChaser.\nSongChaser is a multiplayer game where you try to get from one song to another by finding similar songs with slightly different features. You navigate by these features as defined by Spotify, like \"Danceability\" and \"Acousticness\".\nWe built the base of the program using a K-Nearest Neighbors algorithm trained on Spotify song features, with Euclidean distance, and filtered by genre. This was built using Intel AI PCs, with the remote desktop. Then we built the backend of the site using MongoDB for the database where users are authenticated with their Spotify account so they can play together. This backend was also built with Node.JS and integrated with the KNN models through Flask. Then the frontend is built with React and Bootstrap.\nOne of the big challenges was integrating the machine learning models into the actual website. While the website is written in JavaScript, the machine learning aspect was written in Python. So actually integrating the two of these required us to essentially build two backends, one with Flask and one with Node.JS with Express, so that they could interface with each other. Another big challenge is that we hit the SpotifyAPI rate limit: we were using it too much. So we had to rewrite the backend to rely entirely on a scraped dataset of 10,000 most popular songs instead of referencing anything from the actual Spotify database directly.\nLogging with Spotify was one of our happiest achievements, allowing us to leave space open for more personalization in the future. Connecting such complex backends to a usable website is our greatest joy as well, getting a workable site.\nWe learned a number of new tools from each other, from Flask to React.\nNext would come more personalization for different users, allowing them to export these as playlists or DJ sets with more options.\nMHacks 2024\nSalem Loucks started this project \u2014 6 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    },
    {
        "description": "\n\n\n\n\n\nA puzzle game about an uncooperative calculator. Solve challenges by creating crazy answers whilst mastering the backward functions of an impractical device.\nThe Impractical Catculator\nWe found inspiration from the \"Most Impractical\" award. One of our friends came up with the idea of a calculator that doesn't function properly and makes calculating anything annoying for its users. We loved that idea, so we made it!\nHave you ever wanted to convert numbers into dates? How about converting numbers to emojis? The Impractical Catculator has everything you would never need (and nothing you actually need)!\nWe made The Impractical Catculator using HTML, CSS, and JavaScript.\nAfter the hurdle of figuring out what we wanted to do, the next hurdle was learning the languages we were making it in. Only one member of our team has experience in HTML, CSS, and JavaScript (and it's only a little experience), so we all started with learning how to create basic websites with HTML, CSS, and JavaScript. We also struggled to come up with funny ways the Catculator could toy with your numbers and output random stuff.\nWe successfully implemented all that we wanted to in to the Catculator, and we learned a lot along the way about Web Development. And most of all, we had fun!\nsunhacks\nI worked on the both the back and front end of the project. I implemented a couple of the calculator functions and implemented the artist's graphics into the front end.\nI worked a bit on the front-end but I was mainly in charge of design, aesthetics, and drawing/designing our mascot.\nJacob Kaufman-Warner started this project \u2014 5 days ago\nLeave feedback in the comments!\nLog in or sign up for Devpost to join the conversation."
    }
]